{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951c01d8-18ff-4bf8-a705-997068fd5b4a",
   "metadata": {
    "id": "9e3823ba-1c6f-4b87-849a-8ca8bdc68e5d",
    "tags": []
   },
   "source": [
    "<div> <center><img src=\"https://storage.googleapis.com/open-ff-common/openFF_logo.png\" width=\"100\"/></div>\n",
    "<h1><center>Analysis notebook for JEMA paper</center></h1>\n",
    "<h2><center>\"Increases in Trade Secret Claims in Hydraulic Fracturing Fluids and their Potential Implications for Environmental Health and Water Quality\"</center></h2>\n",
    "    <center><b> Journal of Environmental Management<br>Underhill, et. al. </b></center>\n",
    "    \n",
    "This jupyter notebook contains the code and links to data sets used in the analysis and graphics of this paper.  This notebook should be executable on services such as [Google's Colaboratory](https://colab.google/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3a300-3359-4aa1-b69d-bda64b070ba6",
   "metadata": {
    "id": "af0fd3b5-cdf7-4f5f-9310-0447845d2dbe"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/gwallison/intg_support.git &>/dev/null;\n",
    "!pip install itables  &>/dev/null;\n",
    "!pip install geopandas  &>/dev/null;\n",
    "\n",
    "import urllib\n",
    "urllib.request.urlretrieve('https://storage.googleapis.com/gwa-test/georef-united-states-of-america-county.geojson',\n",
    "                            'counties.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624c93d-35a1-4b11-97ca-dc4e27da779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports custom support code for running this notebook\n",
    "%run intg_support/proprietary_aug_2023.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83b299-e37c-4e4d-8c48-e12905eee009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import gca, mpl\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# get full set of data\n",
    "import intg_support.geo_tools as gt\n",
    "work_dir = ''\n",
    "\n",
    "# this downloads the open-FF data from an online repository.\n",
    "get_fulldf(work_dir=work_dir) \n",
    "df = get_df(os.path.join(work_dir,'full_df.parquet'))\n",
    "\n",
    "# remove records and disclosures that are known duplicates\n",
    "df = df[df.in_std_filtered]\n",
    "\n",
    "# filter to remove disclosures without chemicals\n",
    "df = df[df.ingKeyPresent]\n",
    "\n",
    "# limit the dates\n",
    "df = df[(df.date.dt.year>2013)&(df.date.dt.year<2023)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d449303-e100-4389-942b-1d73161e8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a \"proprietary-only\" data set\n",
    "prop_df = df[df.bgCAS=='proprietary'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de39b04-5ba1-4481-9994-96c5df205ca0",
   "metadata": {},
   "source": [
    "# Identify as many proprietary proppants as we can\n",
    "The specific identities of proprietary records are hidden, but we can use other information to identify the classes of some of them.  Here we try to identify the proppant records among the proprietary records.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe0d51-538f-4e9a-8b69-32daffa70135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"\"\"30/70 Permian\n",
    "aa-400 (aluminum oxide)\n",
    "aluminum oxide\n",
    "amorphous Silica\n",
    "amorphous silica\n",
    "amorphous silicia\n",
    "ceramic microspheres\n",
    "ceramic microspheres/glutaraldehyde\n",
    "ceramic propant\n",
    "ceramic proppant\n",
    "ceramic proppant proprietary\n",
    "copolymer resin fracturing proppant\n",
    "corundum\n",
    "crys4808-60-7talline sio2\n",
    "crystalline cristobalite\n",
    "crystalline silica\n",
    "crystalline silica (quartz)\n",
    "crystalline silica (quartz), proprietary\n",
    "crystalline silica, quartz\n",
    "crystalline silica: cristobalite\n",
    "crystalline silica: quartz (sio2)\n",
    "crystalline silica(quartz),proprietary\n",
    "crystalline sio2\n",
    "fumed silica\n",
    "hydrophobic silica\n",
    "hydrated magnesium silicate\n",
    "magnesium silicate hydrate (talc)\n",
    "meghemite\n",
    "non- crystalline silica (impurity)\n",
    "non-crystalline silica \n",
    "proppant\n",
    "proprietary quartz\n",
    "proprietary silica \n",
    "quartz\n",
    "quartz (sio2)\n",
    "sand\n",
    "silica substrate\n",
    "silica substitute with bonded coatings\n",
    "silicate minerals - ts\n",
    "zinc oxide\"\"\"\n",
    "propp_lst = s.split('\\n')\n",
    "\n",
    "prop_df['is_proppant'] = prop_df.IngredientName.isin(propp_lst)\n",
    "print(f'Total number of identified proprietary proppant records: {prop_df.is_proppant.sum()}')\n",
    "prop_df[prop_df.is_proppant].IngredientName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ef232-7dd9-4084-90c2-39d7cf08bceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove one obvious error point that is > 100,000,000 pounds\n",
    "import numpy as np\n",
    "prop_df.calcMass = np.where(prop_df.calcMass>100000000,np.NaN,prop_df.calcMass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007096c8-30e3-47ff-b30e-80d841fd9b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pylab import gca, mpl\n",
    "ax = prop_df[prop_df.is_proppant].plot('date','calcMass',style='o',alpha=.5,legend=False)\n",
    "ax.set_title('Mass of proprietary records with proppant ingredient name')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Mass (lbs.) of individual proprietary records')\n",
    "ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb8149-1cf0-4d78-a1cf-9698ae62a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def my_formatter(x, pos):\n",
    "     return \"{}\".format('{x:,.4f}' if x<1 else '{x:,.0f}' )\n",
    "    \n",
    "sns.set_theme(style=\"ticks\")\n",
    "prop_df['logMass'] = np.where(prop_df.calcMass>0.00001,np.log10(prop_df.calcMass),np.NaN) \n",
    "prop_df['year'] = prop_df.date.dt.year\n",
    "\n",
    "ax = sns.boxplot(data=prop_df[(~prop_df.is_proppant)&(prop_df.calcMass>.001)], x='year',y='calcMass',\n",
    "                 showfliers=False, color='skyblue')\n",
    "\n",
    "ax.set_ylabel('log of calculated mass')\n",
    "plt.yscale(\"log\")\n",
    "ax.tick_params(labelright=True, right=True, which='both')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a838d4f-a555-42c4-b994-4b2fea1251de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show table and summary regression of mass by year\n",
    "\n",
    "prop_df['year'] = prop_df.date.dt.year\n",
    "gb = prop_df[~prop_df.is_proppant].groupby('year',as_index=False)['calcMass'].agg(['median','mean','sum','count']).reset_index()\n",
    "import scipy.stats as stats\n",
    "print(stats.linregress(gb.year,gb['mean']),'\\n')\n",
    "sns.regplot(data=gb,x='year',y='mean',scatter=True)\n",
    "nonproppantMass = gb.copy()\n",
    "gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc3839-9e11-48f0-a077-0028e6b798b3",
   "metadata": {},
   "source": [
    "# Basic stats of proprietary records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7aaa7-ceb5-4f68-93eb-a6da0f039741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Total calculated mass of proprietary records: {round_sig(prop_df.calcMass.sum(),3)} lbs.')\n",
    "prop_df['added_year'] = prop_df.date_added.dt.year\n",
    "print(f'Calculated mass of proprietary records added in 2023: {round_sig(prop_df[prop_df.added_year==2023].calcMass.sum(),3)} lbs.')\n",
    "\n",
    "prop_df['year'] = prop_df.date.dt.year\n",
    "# with pd.option_context(\"display.float_format\", \"{:,.0f}\".format):\n",
    "gb = prop_df.groupby('year',as_index=False)['calcMass'].sum()\n",
    "gb['rounded'] = gb.calcMass.map(lambda x: round_sig(x,3))\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3561a45-f0c6-4103-94a0-a54835aa2b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_num_disc = df.UploadKey.unique().size\n",
    "num_disc_with_prop = prop_df.UploadKey.unique().size\n",
    "print(f'Total num of disclosures in set:      {total_num_disc:,}')\n",
    "print(f'Total with at least one propriertary: {num_disc_with_prop:,}')\n",
    "print(f'Overall Percent with proprietary:     {num_disc_with_prop/total_num_disc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d346b-1e5e-4a82-a608-a57b5d142151",
   "metadata": {
    "tags": []
   },
   "source": [
    "# show all the Purpose fields\n",
    "You could also look at the [Browser's version](https://storage.googleapis.com/open-ff-browser/proprietary/analysis_proprietary.html#raw) of this.  Extra long purposes (using indicating multiple products) are truncated and combined.  They are not very useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5f131-a0c1-4658-b82a-ab4b9bf10dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prop_df.groupby('Purpose',as_index=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ca8cf-aa54-47fa-9d0e-aab42f87065d",
   "metadata": {},
   "source": [
    "# make graphs of operators and suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d076728-eb6d-45a4-b07c-4ea02a24b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# start with the basic % proprietary for each operator\n",
    "\n",
    "\n",
    "gb_full = df.groupby(['bgOperatorName','UploadKey'],as_index=False).size()\n",
    "gb_full = gb_full.groupby('bgOperatorName')['UploadKey'].count().reset_index()\n",
    "gb_full.columns = ['bgOperatorName','disclosure_cnt_all']\n",
    "\n",
    "gb_prop = prop_df.groupby(['bgOperatorName','UploadKey'],as_index=False).size()\n",
    "gb_prop = gb_prop.groupby('bgOperatorName')['UploadKey'].count().reset_index()\n",
    "gb_prop.columns = ['bgOperatorName','disclosure_cnt_prop']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac63e1-5268-4e0f-8c45-1667ca50a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mg = pd.merge(gb_full,gb_prop,on='bgOperatorName',how='left')\n",
    "# mg = pd.merge(mg,gb1,on='bgOperatorName',how='left')\n",
    "# mg = pd.merge(mg,newmg.reset_index(),on='op_common',how='left')\n",
    "# mg = mg.rename({'UploadKey':'num_prop_records'},axis=1)\n",
    "# mg = mg[mg.disclosure_cnt_prop>0]\n",
    "# mg.disclosure_cnt_prop.fillna(0,inplace=True)\n",
    "# mg['prop_perc'] = mg.disclosure_cnt_prop/mg.disclosure_cnt_all * 100\n",
    "# mg.sort_values('disclosure_cnt_all',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303f9a6-e409-4186-a3a1-66fa8394dda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cas = prop_df.bgCAS.iloc[0]\n",
    "# use the most common name given in FF for the label\n",
    "gb1 = prop_df.groupby('bgOperatorName')['OperatorName'].agg(lambda x: x.value_counts().index[0])\n",
    "gb1 = gb1.reset_index()\n",
    "gb1.columns = ['bgOperatorName','op_common']\n",
    "mg = pd.merge(prop_df,gb1,on='bgOperatorName',how='left')\n",
    "newmg = mg.groupby('op_common')['UploadKey'].count().sort_values(ascending=False)\n",
    "\n",
    "cmg = pd.merge(gb_full,gb_prop,on='bgOperatorName',how='left')\n",
    "cmg = pd.merge(cmg,gb1,on='bgOperatorName',how='left')\n",
    "cmg = pd.merge(cmg,newmg.reset_index(),on='op_common',how = 'right')#how='left')\n",
    "cmg = cmg.rename({'UploadKey':'num_prop_records'},axis=1)\n",
    "cmg = cmg[cmg.disclosure_cnt_prop>0]\n",
    "cmg.disclosure_cnt_prop.fillna(0,inplace=True)\n",
    "cmg['prop_perc'] = cmg.disclosure_cnt_prop/cmg.disclosure_cnt_all * 100\n",
    "cmg = cmg.sort_values('disclosure_cnt_all',ascending=False)\n",
    "cmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8befa0-08b6-4a84-9374-f40895c278a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotmg = cmg[['op_common','prop_perc','disclosure_cnt_all','num_prop_records']].sort_values('num_prop_records',ascending=False)\n",
    "ax = plotmg[:15][['op_common','num_prop_records']].plot.barh(figsize=(7,6),\n",
    "                                                             legend=None,xlim=(0,35000))\n",
    "ax.tick_params(axis=\"y\", labelsize=14)\n",
    "ax.tick_params(axis=\"x\", labelsize=14)\n",
    "plt.xlabel('Number of records',fontsize=16);\n",
    "plt.ylabel('Operating Company',fontsize=16);\n",
    "ax.set_yticklabels(plotmg[:15].op_common)\n",
    "plt.title(f'Number of \"proprietary\" records, by operator',fontsize=16);\n",
    "\n",
    "perc_lst = plotmg[:15].prop_perc.tolist()\n",
    "for i,p in enumerate(ax.patches):\n",
    "    width = p.get_width()\n",
    "    #nw = f'  {round_sig(width,8)}'\n",
    "    nw = f'  {float(round(perc_lst[i],1))}%'\n",
    "    plt.text(p.get_width(), p.get_y()+0.55*p.get_height(),\n",
    "             nw,\n",
    "             ha='left', va='center',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215a43b-6ac3-483b-b6f2-bd1c98a0f617",
   "metadata": {},
   "source": [
    "# Suppliers\n",
    "\n",
    "Here we need to remove records that are systems approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481b5b7-18ac-44e0-afef-5eb1db8c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.CASNumber.str.lower().str.contains('listed below')\n",
    "c1 = df.IngredientName.str.lower().str.contains('listed below')\n",
    "print(f'Number of disclosures that are systems approach removed from consideration : {df[c1|c].UploadKey.unique().size}')\n",
    "upk = df[c1|c].UploadKey.unique().tolist()\n",
    "prop_df_sup = prop_df[~prop_df.UploadKey.isin(upk)]\n",
    "print(f'Number of disclosures in the supplier analysis: {prop_df_sup.UploadKey.unique().size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec57622-e9cc-43c0-971e-623a2eda5d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# records to exclude\n",
    "not_comp = ['MISSING','Listed Above']\n",
    "\n",
    "gb1 = prop_df_sup.groupby('bgSupplier')['Supplier'].agg(lambda x: x.value_counts().index[0])\n",
    "gb1 = gb1.reset_index()\n",
    "gb1.columns = ['bgSupplier','sup_common']\n",
    "mg = pd.merge(prop_df_sup,gb1,on='bgSupplier',how='left')\n",
    "\n",
    "mg = mg[~mg.sup_common.isin(not_comp)]\n",
    "\n",
    "ax = mg.groupby('sup_common')['UploadKey'].count()\\\n",
    "         .sort_values(ascending=False)[:15].plot.barh(figsize=(7,6))\n",
    "ax.tick_params(axis=\"y\", labelsize=14)\n",
    "ax.tick_params(axis=\"x\", labelsize=14)\n",
    "plt.xlabel('Number of records',fontsize=16);\n",
    "plt.ylabel('Supplier Company',fontsize=16);\n",
    "plt.title(f'Number of records declared proprietary, by supplier',fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95469e-3e58-4855-8039-e907da02379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_full = df.groupby(['bgSupplier'],as_index=False).size()\n",
    "gb_full.columns = ['bgSupplier','record_cnt_all']\n",
    "\n",
    "gb_prop = prop_df_sup.groupby(['bgSupplier'],as_index=False).size()\n",
    "gb_prop.columns = ['bgSupplier','record_cnt_prop']\n",
    "\n",
    "# gb_precs = prop_df.groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40861fb9-ad9e-4c1d-860c-6ec65bf1d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = pd.merge(gb_full,gb_prop,on='bgSupplier',how='left')\n",
    "mg = pd.merge(mg,gb1,on='bgSupplier',how='left')\n",
    "mg.record_cnt_prop.fillna(0,inplace=True)\n",
    "mg['prop_perc'] = mg.record_cnt_prop/mg.record_cnt_all * 100\n",
    "mg.sort_values('record_cnt_all',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b000f-4e87-4af1-a888-4a2a4ef34b9f",
   "metadata": {},
   "source": [
    "# MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b15ce9-99fd-43ad-898a-19a142c7aec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "import numpy as np\n",
    "\n",
    "import branca.colormap as cm\n",
    "linear = cm.LinearColormap(['green','yellow','red'], vmin=3., vmax=10.)\n",
    "linear\n",
    "\n",
    "def fix_county_names(df):\n",
    "    trans = {'mckenzie':'mc kenzie',\n",
    "             'dewitt':'de witt',\n",
    "             'mcclain':'mc clain',\n",
    "             'mcintosh':'mc intosh',\n",
    "             'mckean':'mc kean',\n",
    "             'mcmullen':'mc mullen'}\n",
    "    for wrong in trans.keys():\n",
    "        df.CountyName = np.where(df.CountyName==wrong,trans[wrong],df.CountyName)\n",
    "    return df\n",
    "\n",
    "def create_county_choropleth(data,\n",
    "                             start_loc=[40, -96],start_zoom = 6,\n",
    "                             custom_scale = [], plotlog = True,\n",
    "                             legend_name = 'Test legend',\n",
    "                             show_only_data_states=True,\n",
    "                             fill_color = 'YlOrRd',\n",
    "                             #popup_enabled=True, tooltip_enabled=False,\n",
    "                             fields = ['CountyName','orig_value'],\n",
    "                             aliases = ['County: ','data: ']):\n",
    "    fn = r\"counties.geojson\"\n",
    "    if len(data)<1:\n",
    "        print('No mappable data')\n",
    "        return\n",
    "    geojson = gpd.read_file(fn)\n",
    "    data['orig_value'] = data.value\n",
    "\n",
    "    geojson['StateName'] = geojson.ste_name.str.lower()\n",
    "    geojson['CountyName'] = geojson.coty_name.str.lower()\n",
    "    geojson = fix_county_names(geojson)\n",
    "    working = geojson[['StateName','CountyName','coty_code','geometry']]\n",
    "    #geojson = geojson.to_crs(5070)\n",
    "    working = pd.merge(working,data,on=['StateName','CountyName'],how='left')\n",
    "    #print(geojson.info())\n",
    "    if start_loc==[]:\n",
    "        start_loc = [geojson.geometry.centroid.x.mean(),geojson.geometry.centroid.y.mean()]\n",
    "    f = folium.Figure(width=800, height=500)\n",
    "    m = folium.Map(location= start_loc, tiles=\"openstreetmap\",\n",
    "                   zoom_start=start_zoom).add_to(f)\n",
    "    if plotlog:\n",
    "        working.value = np.log10(working.value+1)\n",
    "        legend_name = legend_name + ' (log transformed)'\n",
    "    working.orig_value.fillna('no data',inplace=True)\n",
    "    \n",
    "    if custom_scale==[]:\n",
    "        custom_scale = (working['value'].quantile((0,0.2,0.4,0.6,0.8,1))).tolist()\n",
    "    if show_only_data_states:\n",
    "        gb = data.groupby(['StateName','CountyName'],as_index=False)['value'].first()\n",
    "        datalst = []\n",
    "        for i,row in gb.iterrows():\n",
    "            datalst.append((row.StateName,row.CountyName))\n",
    "        wlst = []\n",
    "        working['tup'] = list(zip(working.StateName.tolist(),working.CountyName.tolist()))\n",
    "        geojson['tup'] = list(zip(geojson.StateName.tolist(),geojson.CountyName.tolist()))\n",
    "        \n",
    "#         working = working[working.StateName.isin(data.StateName.unique().tolist())]\n",
    "#         geojson = geojson[geojson.StateName.isin(data.StateName.unique().tolist())]\n",
    "#         c1 = working.CountyName.isin(data.CountyName.unique().tolist())\n",
    "#         c2 = working.StateName.isin(data.StateName.unique().tolist())\n",
    "#         c3 = geojson.CountyName.isin(data.CountyName.unique().tolist())\n",
    "#         c4 = geojson.StateName.isin(data.StateName.unique().tolist())\n",
    "        working = working[working.tup.isin(datalst)]\n",
    "        geojson = geojson[geojson.tup.isin(datalst)]\n",
    "    working.StateName = working.StateName.str.title()\n",
    "    working.CountyName = working.CountyName.str.title()\n",
    "    #print(f'States in geojson: {working.StateName.unique().tolist()}')\n",
    "\n",
    "    linear = cm.LinearColormap(['green','yellow','red'], vmin=3., vmax=10.)\n",
    "    linear\n",
    "    \n",
    "    folium.Choropleth(\n",
    "                geo_data=geojson,\n",
    "                data=working,\n",
    "                columns=['coty_code', 'value'],  #Here we tell folium to get the fips and plot values for each state\n",
    "                key_on='feature.properties.coty_code',\n",
    "                threshold_scale=custom_scale, #use the custom scale we created for legend\n",
    "                #fill_color='YlOrRd',\n",
    "                fill_color=fill_color,\n",
    "                nan_fill_color=\"gainsboro\", #Use white color if there is no data available for the area\n",
    "                fill_opacity=0.7,\n",
    "                line_opacity=0.4,\n",
    "                line_weight=0.4,\n",
    "                legend_name= legend_name, #title of the legend\n",
    "                highlight=True,\n",
    "                line_color='black').add_to(m) \n",
    "    \n",
    "    folium.features.GeoJson(\n",
    "                data=working,\n",
    "                name='',\n",
    "                smooth_factor=2,\n",
    "                style_function=lambda x: {'color':'black','fillColor':'transparent','weight':0.5},\n",
    "                popup=folium.features.GeoJsonPopup(\n",
    "                    fields=fields,\n",
    "                    aliases=aliases, \n",
    "                    localize=True,\n",
    "                    sticky=False,\n",
    "                    labels=True,\n",
    "                    style=\"\"\"\n",
    "                        background-color: #F0EFEF;\n",
    "                        border: 2px solid black;\n",
    "                        border-radius: 3px;\n",
    "                        box-shadow: 3px;\n",
    "                    \"\"\",\n",
    "                    max_width=800,),\n",
    "                        highlight_function=lambda x: {'weight':3,'fillColor':'grey'},\n",
    "                    ).add_to(m)  \n",
    "\n",
    "    \n",
    "\n",
    "# fit_bounds needs work: https://stackoverflow.com/questions/58162200/pre-determine-optimal-level-of-zoom-in-folium\n",
    "#     sw = data[['bgLatitude', 'bgLongitude']].min().values.tolist()\n",
    "#     ne = data[['bgLatitude', 'bgLongitude']].max().values.tolist()\n",
    "\n",
    "#     m.fit_bounds([sw, ne]) \n",
    "    display(f)\n",
    "\n",
    "\n",
    "def CountyMap(df):\n",
    "    state_list = df.bgStateName.unique().tolist()\n",
    "    #start_loc = get_geog_center(state_list)\n",
    "    #print(statename,start_loc)\n",
    "    cond = (df.loc_within_state=='YES')&(df.loc_within_county=='YES')\n",
    "    if cond.sum()==0:  # no valid fracks for this state\n",
    "        display(md('## No mappable fracks for this operator!'))\n",
    "        # display(md(f'Any data in this state set may be labeled incorrectly as {statename}'))\n",
    "        return\n",
    "    gb = df[cond].groupby(['bgStateName','bgCountyName',\n",
    "                                                   'UploadKey'],as_index=False)['bgCAS'].count()\n",
    "    gb = gb.groupby(['bgStateName','bgCountyName'],as_index=False)['UploadKey'].count().rename({'bgStateName':'StateName',\n",
    "                                                                                                'bgCountyName':'CountyName',\n",
    "                                                                                                'UploadKey':'value'},\n",
    "                                                                                                axis=1)    \n",
    "    zoom = 3.6\n",
    "    create_county_choropleth(gb,plotlog=True,#plotlog=True,\n",
    "                             custom_scale= [0,1,2,3,4],\n",
    "                             #start_loc=start_loc, # center of state's data\n",
    "                             legend_name='Number of FracFocus disclosures',\n",
    "                             start_zoom=zoom,fields=['StateName','CountyName','orig_value'],\n",
    "                             aliases=['State: ','County: ','Number Fracking disclosures: '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccbaa2-b4c7-4d40-b50e-c57b97756185",
   "metadata": {},
   "source": [
    "## Map by number of disclosures\n",
    "Does this color scheme support our points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8a538-0fee-42cd-833e-6616dd419448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CountyMap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bea3e6-f278-4e04-8d6e-a283b538b101",
   "metadata": {},
   "source": [
    "# make map with proprietary fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739ca30-3424-431a-89c5-6bfc06db5515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first make data frame with number of fracks by county\n",
    "gb_all = df.groupby(['bgStateName','bgCountyName','UploadKey'],as_index=False).size()\n",
    "gb_all = gb_all.groupby(['bgStateName','bgCountyName'],as_index=False)['UploadKey'].count()                    \n",
    "# gb_all\n",
    "\n",
    "# now make data frame with number of fracks with at least one prop. chem by county\n",
    "gb_prop = prop_df.groupby(['bgStateName','bgCountyName','UploadKey'],as_index=False).size()\n",
    "gb_prop = gb_prop.groupby(['bgStateName','bgCountyName'],as_index=False)['UploadKey'].count()                    \n",
    "gb_prop.columns = ['bgStateName','bgCountyName','UploadKey_prop']\n",
    "gb_prop\n",
    "\n",
    "mg = pd.merge(gb_all,gb_prop,on=['bgStateName','bgCountyName'],how='left')\n",
    "mg.UploadKey_prop.fillna(0,inplace=True)\n",
    "mg['perc_prop'] = mg.UploadKey_prop/mg.UploadKey *100\n",
    "#mg[mg.UploadKey>0].perc_prop.hist(bins=5)\n",
    "\n",
    "mg['prop_dev'] = mg.perc_prop - 82\n",
    "\n",
    "mg.UploadKey_prop.sum()/mg.UploadKey.sum() *100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f44d6-5fd6-4143-987d-b171ec3c5a3a",
   "metadata": {},
   "source": [
    "## %disclosures_with_proprietary: 4 simple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79a020-5040-4cd3-8104-58533f2c25de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CountyPropMap(df):\n",
    "    state_list = df.bgStateName.unique().tolist()\n",
    "    zoom = 3.6\n",
    "    df['value'] = df.perc_prop\n",
    "    # df['value'] = df.prop_dev\n",
    "    df['StateName'] = df.bgStateName\n",
    "    df['CountyName'] = df.bgCountyName\n",
    "    create_county_choropleth(df,plotlog=False,\n",
    "                             custom_scale=[0,25,50,75,100],\n",
    "                             fill_color='RdBu_r',\n",
    "                             #start_loc=start_loc, # center of state's data\n",
    "                             legend_name='Percent of disclosures with at least one proprietary record',\n",
    "                             start_zoom=zoom,fields=['bgStateName','bgCountyName','orig_value','UploadKey'],\n",
    "                             aliases=['State: ','County: ','% disc with proprietary: ','total num of disclosures'])\n",
    "    \n",
    "CountyPropMap(mg[mg.UploadKey>5].copy()) # county must have more than 5 FF disclosures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3d6db-bd7c-4763-844b-b182dacf15b3",
   "metadata": {},
   "source": [
    "# Compare to Trickey 2020\n",
    "state-wide proprietary rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6da0c3-941f-4ecb-82e6-6100f7b19fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gb_all = df.groupby(['bgStateName','UploadKey'],as_index=False).size()\n",
    "gb_all = gb_all.groupby('bgStateName',as_index=False)['UploadKey'].count()\n",
    "gb_all.columns = ['bgStateName','num_all_disc']\n",
    "\n",
    "gb_prop = df[df.bgCAS=='proprietary'].groupby(['bgStateName','UploadKey'],as_index=False).size()\n",
    "gb_prop = gb_prop.groupby('bgStateName',as_index=False)['UploadKey'].count()\n",
    "gb_prop.columns = ['bgStateName','num_prop_disc']\n",
    "\n",
    "mg = pd.merge(gb_all,gb_prop,on='bgStateName',how='left')\n",
    "mg.num_prop_disc.fillna(0,inplace=True)\n",
    "\n",
    "mg['state_prop_percent'] = mg.num_prop_disc/mg.num_all_disc *100\n",
    "mg.sort_values('num_all_disc',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a7285-9d0c-4ad4-90a6-b12b03c35151",
   "metadata": {},
   "source": [
    "# ambiguousID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd5f07-689b-4e4e-9624-ce91eac0fc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amb_df = df[df.bgCAS=='ambiguousID'].copy()\n",
    "amb_df['has_mass'] = amb_df.calcMass>0\n",
    "amb_df.has_mass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc18b6-4a89-467c-bc06-5ea5e62e996a",
   "metadata": {},
   "source": [
    "## what are the ambiguousID `IngredientName`s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec52b5-ae78-4f27-8332-11bcef7973ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amb_df[amb_df.has_mass].IngredientName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b039b1-604d-48c7-81a4-0f8324500930",
   "metadata": {},
   "source": [
    "## what are the ambiguousID `IngredientName`s  (Just the big ones)\n",
    "Mostly water-type things.  A handful of proppants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca9471-e7c8-47ef-99fe-84d22ec39484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amb_df[amb_df.calcMass>100000].IngredientName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a93a3-c621-4f5a-b11a-5a4e40daf821",
   "metadata": {},
   "source": [
    "# AmbiguousID Proppants: Number and mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8a0d0-210d-4f4f-b5ce-15f8038870a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proppants = ['silica substrate',\n",
    " 'remainder is made up of various other oxides and trace elements, of which cao, mgo, and fe2o3 are the largest percentages',\n",
    " 'cyrstalline silica','mix of various oxides (cao, mgo, and fe2o4','resin coated fracturing proppant',\n",
    " '40/70 ppc','mix of various oxides (cao, mgo, and fe2o3',\n",
    " 'mix of various oxides (cao, mgo, and fe203','mix of various oxides (cao, mgo, and fe204',\n",
    " 'crystalline silica, quartz','100 mesh sand','ceramic',\n",
    " '20/40 pc','crystalline silica,quartz','aluminum oxide','nfidb:sand-200 mesh silica',\n",
    " 'nfidb:200 mesh ssa-1','non-crystalline silica (impurity)',\n",
    " 'silica in form of quartz','40/70 white','mulite']\n",
    "c = amb_df.IngredientName.isin(proppants)\n",
    "\n",
    "print(f'Number of ambiguousID proppants = {len(amb_df[c])}; calculated mass= {round(amb_df[c].calcMass.sum(),0):,} lbs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f77b4b-dd2b-4916-b2b0-58a1e26e4cc1",
   "metadata": {},
   "source": [
    "## Likely water records in ambiguousID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e322034-827a-4663-9376-c45610aa6859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = amb_df.IngredientName.str.contains('water')\n",
    "\n",
    "print(f'Number of ambiguousID **water** = {len(amb_df[c])}; calculated mass= {round(amb_df[c].calcMass.sum(),0):,} lbs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f65632-e4a1-479c-bffb-c96349ce6e82",
   "metadata": {},
   "source": [
    "# Proprietary plot (added back 8/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9c4d9-a55f-4591-96b6-71fd421a634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "def proprietary_plot(df,plot_title='TEST',minyr=2014,maxyr=2022):\n",
    "    # df = df.copy()\n",
    "    df['year'] = df.date.dt.year\n",
    "    df = df[df.year<=maxyr]\n",
    "    df = df[df.year>=minyr]\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['propCut'] = pd.cut(mg.percProp,right=False,bins=[0,0.0001,10,25,50,101],\n",
    "                          labels=['no proprietary claims','up to 10% proprietary claims',\n",
    "                                  'between 10 and 25% proprietary claims',\n",
    "                                  'between 25 and 50% proprietary claims',\n",
    "                                  'greater than 50% proprietary claims'])\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    # tmp = mg.groupby(['year','propCut'],as_index=False).size()\n",
    "    # print(tmp.pivot(index='year',columns='propCut',values='size'))\n",
    "    \n",
    "    out = mg.drop(['date','UploadKey'],axis=1)\n",
    "    t = out[out.numvalid>0].groupby(['year','propCut'],as_index=False)['numvalid'].count()\n",
    "    sums = t.groupby('year',as_index=False)['numvalid'].sum().rename({'numvalid':'tot'},axis=1)\n",
    "    t = pd.merge(t,sums,on='year',how='left')\n",
    "    t['PercentProp'] = t.numvalid/t.tot *100\n",
    "\n",
    "    piv = t.pivot(index='year', columns='propCut', values='PercentProp')\n",
    "    \n",
    "    with pd.option_context(\"display.float_format\", \"{:,.1f} %\".format):\n",
    "        iShow(piv)\n",
    "\n",
    "    ax = piv.plot.area(figsize=(12,7),ylim=(0,100),xlim=(minyr,maxyr),colormap='Reds')\n",
    "    ax.set_title(f'Percentage of valid records that are Trade Secret claims at the disclosure level', fontsize=16)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[::-1], labels[::-1], title='Disclosure Proprietary\\nPercentage class\\n',\n",
    "              loc='upper left',bbox_to_anchor=(1, 1))\n",
    "    ax.set_ylabel('Percentage of disclosures', fontsize=16)\n",
    "    ax.set_xlabel('Year', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.suptitle(f'{plot_title}',fontsize=24)\n",
    "\n",
    "    gb = df.groupby(['year','UploadKey'],as_index=False)['bgCAS'].count()\n",
    "    gb = gb.groupby('year',as_index=False)['UploadKey'].count()#.rename({'UploadKey':'number of disclosures'},axis=1)\n",
    "    s = 'Number of disclosures by year:\\n\\n'\n",
    "    for i,row in gb.iterrows():\n",
    "        s+= f'   {row.year}: {row.UploadKey:7,} \\n'\n",
    "    at2 = AnchoredText(s,\n",
    "                       loc='lower left', prop=dict(size=10), frameon=False,\n",
    "                       bbox_to_anchor=(1., 0.),\n",
    "                       bbox_transform=ax.transAxes\n",
    "                       )\n",
    "    at2.patch.set_boxstyle(\"square,pad=0.\")\n",
    "    ax.add_artist(at2)\n",
    "\n",
    "    \n",
    "# test = 'pennsylvania'\n",
    "# variable = 'bgStateName'\n",
    "testtitle = 'Trade Secret frequency'\n",
    "proprietary_plot(df,testtitle,minyr=2014,maxyr=2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d298652-fdcf-4ff8-879e-e1b15efc5a83",
   "metadata": {},
   "source": [
    "###  Break up the categories for plotting and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e3951-c024-4f30-8aa1-bb2a97d2cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def new_plot(df, category='no proprietary designations'):\n",
    "    df['year'] = df.date.dt.year\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['propCut'] = pd.cut(mg.percProp,right=False,bins=[0,0.0001,10,25,50,101],\n",
    "                          labels=['no proprietary designations','up to 10% proprietary designations',\n",
    "                                  'between 10 and 25% proprietary designations',\n",
    "                                  'between 25 and 50% proprietary designations',\n",
    "                                  'greater than 50% proprietary designations'])\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    \n",
    "    out = mg.drop(['date','UploadKey'],axis=1)\n",
    "    t = out[out.numvalid>0].groupby(['year','propCut'],as_index=False)['numvalid'].count()\n",
    "    sums = t.groupby('year',as_index=False)['numvalid'].sum().rename({'numvalid':'tot'},axis=1)\n",
    "    t = pd.merge(t,sums,on='year',how='left')\n",
    "    t['PercentProp'] = t.numvalid/t.tot *100\n",
    "\n",
    "    g = sns.FacetGrid(t,col='propCut',col_wrap=2,height=4)\n",
    "    g.map(sns.regplot,'year','PercentProp')\n",
    "    for cat in mg.propCut.unique().tolist()[:-1]:\n",
    "        print(f'Regression stats for <{cat}>')\n",
    "        subdf = t[t.propCut==cat]\n",
    "        print(stats.linregress(subdf.year,subdf.PercentProp),'\\n')\n",
    "new_plot(df,'no proprietary designations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e19216-d1f5-4280-adc7-af4529c0065f",
   "metadata": {},
   "source": [
    "### Try analysis of just mean proportion over years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb42eb-3abb-4824-9f77-6f95ec486289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_plot(df):\n",
    "    df['year'] = df.date.dt.year\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    ygb = mg.groupby('year',as_index=False)['percProp'].mean()\n",
    "\n",
    "    print(f'Regression stats')\n",
    "    print(stats.linregress(ygb.year,ygb.percProp),'\\n')\n",
    "    sns.regplot(data=ygb,x='year',y='percProp',scatter=True)\n",
    "single_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a844132-b0ce-4e14-a07b-f42b248c0ebe",
   "metadata": {},
   "source": [
    "#### analaysis of \"no\" vs. \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514f0de-83be-454b-a6e6-622df9f970c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def yes_v_no(df):\n",
    "    df['year'] = df.date.dt.year\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['propCut'] = pd.cut(mg.percProp,right=False,bins=[0,0.0001,101],\n",
    "                          labels=['no proprietary designations',\n",
    "                                  'at least one proprietary record'])\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    \n",
    "    out = mg.drop(['date','UploadKey'],axis=1)\n",
    "    t = out[out.numvalid>0].groupby(['year','propCut'],as_index=False)['numvalid'].count()\n",
    "    sums = t.groupby('year',as_index=False)['numvalid'].sum().rename({'numvalid':'tot'},axis=1)\n",
    "    t = pd.merge(t,sums,on='year',how='left')\n",
    "    t['PercentProp'] = t.numvalid/t.tot *100\n",
    "\n",
    "    g = sns.FacetGrid(t,col='propCut',col_wrap=2,height=4)\n",
    "    g.map(sns.regplot,'year','PercentProp')\n",
    "    for cat in mg.propCut.unique().tolist()[:-1]:\n",
    "        print(f'Regression stats for <{cat}>')\n",
    "        subdf = t[t.propCut==cat]\n",
    "        print(stats.linregress(subdf.year,subdf.PercentProp),'\\n')\n",
    "yes_v_no(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5bf42-f613-40c2-bb1d-713e65ff7563",
   "metadata": {},
   "source": [
    "#### proprietary bars - all years lumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9bcee-5a81-4e74-ba5f-c26285842554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def proprietary_bars(df,plot_title='TEST'):\n",
    "    df = df.copy()\n",
    "    df['year'] = df.date.dt.year\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fillb\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['propCut'] = pd.cut(mg.percProp,right=False,bins=[0,0.0001,10,25,50,101],\n",
    "                          labels=['no proprietary designations','up to 10% of records\\nare proprietary designations',\n",
    "                                  'between 10 and 25% of records\\nare proprietary designations',\n",
    "                                  'between 25 and 50% of records\\nare proprietary designations',\n",
    "                                  'greater than 50% of records\\nare proprietary designations'])\n",
    "    \n",
    "    # mg.propCut.value_counts(sort=False).plot(kind='barh',colormap='Reds')\n",
    "    \n",
    "    import seaborn as sns\n",
    "    t = mg.propCut.value_counts(sort=False).reset_index()\n",
    "    totcnt = t.propCut.sum()\n",
    "    t['prop_perc'] = t.propCut/totcnt *100\n",
    "    ax = sns.barplot(data=t,y='index',x='propCut',palette='Reds',orient=\"h\")\n",
    "    ax.set_xlabel(\"Number of disclosures\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlim(right=58000)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    perc_lst = t.prop_perc.tolist()\n",
    "    for i,p in enumerate(ax.patches):\n",
    "        width = p.get_width()\n",
    "        #nw = f'  {round_sig(width,8)}'\n",
    "        nw = f'  {float(round(perc_lst[i],1))}%'\n",
    "        plt.text(p.get_width(), p.get_y()+0.55*p.get_height(),\n",
    "                 nw,\n",
    "                 ha='left', va='center',fontsize=12)\n",
    "proprietary_bars(df,plot_title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002dc7b6-b494-4db0-9e53-355dc1ba7d60",
   "metadata": {},
   "source": [
    "# Factors that should influence changes of proprietary mass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927d58a-a2a7-41c6-b762-2df6c0a18ddf",
   "metadata": {},
   "source": [
    "### Calculate the percent contribution to a job of all proprietary (non-proppant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3217f01-76ba-47f3-83ca-bef581bcc0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c1 =  ~df.IngredientName.isin(propp_lst)\n",
    "c2 = df.bgCAS=='proprietary'\n",
    "gbp = df[c1&c2].groupby(['UploadKey','year'],as_index=False)['PercentHFJob'].sum()\n",
    "gball = df.groupby(['UploadKey','year'],as_index=False)['PercentHFJob'].sum().rename({'PercentHFJob':'totHFJob'},axis=1)\n",
    "mg = pd.merge(gball,gbp,on=['UploadKey','year'],how='left')\n",
    "mg.PercentHFJob.fillna(0,inplace=True)\n",
    "# filter out the bad\n",
    "c1 = mg.totHFJob>95\n",
    "c2 = mg.totHFJob<105\n",
    "mg = mg[c1&c2]\n",
    "gb = mg.groupby('year', as_index=False)['PercentHFJob'].mean()\n",
    "\n",
    "print(stats.linregress(gb.year,gb.PercentHFJob),'\\n')\n",
    "sns.regplot(data=gb,x='year',y='PercentHFJob',scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd69ee8-9990-480c-bccd-5ee0d2ad1f9b",
   "metadata": {},
   "source": [
    "## Oct 2023.  Median water volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06392be-b25b-4cb3-9914-61677e599e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import log10, floor\n",
    "def round_sig(x, sig=2,guarantee_str=''):\n",
    "    try:\n",
    "        if abs(x)>=1:\n",
    "            out =  int(round(x, sig-int(floor(log10(abs(x))))-1))\n",
    "            return f\"{out:,d}\" # does the right thing with commas\n",
    "        else: # fractional numbers\n",
    "            return str(round(x, sig-int(floor(log10(abs(x))))-1))\n",
    "    except: \n",
    "        if guarantee_str:\n",
    "            return guarantee_str\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ebb69-21b6-4684-bedb-16442e95227a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pylab import gca, mpl\n",
    "df['year'] = df.date.dt.year\n",
    "gb = df.groupby(['year','UploadKey'],as_index=False)['TotalBaseWaterVolume'].first()\n",
    "gb1 = gb.groupby('year',as_index=False)['TotalBaseWaterVolume'].median()\n",
    "gb1.plot('year','TotalBaseWaterVolume',ylim=(0,18000000),title='Median volume water by year')\n",
    "ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "gb1['rounded'] = gb1.TotalBaseWaterVolume.map(lambda x: round_sig(x,3))\n",
    "\n",
    "print(stats.linregress(gb1.year,gb1.TotalBaseWaterVolume),'\\n')\n",
    "sns.regplot(data=gb1,x='year',y='TotalBaseWaterVolume',scatter=True)\n",
    "\n",
    "gb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77db05-0126-4213-aebd-7b43c37a2b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pylab import gca, mpl\n",
    "df['year'] = df.date.dt.year\n",
    "gb = df.groupby(['year','UploadKey'],as_index=False)['job_mass'].first()\n",
    "gb1 = gb.groupby('year',as_index=False)['job_mass'].median()\n",
    "gb1.plot('year','job_mass',#ylim=(0,16500000),\n",
    "         title='Median total mass of fracking job by year')\n",
    "ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "gb1['rounded'] = gb1.job_mass.map(lambda x: round_sig(x,3))\n",
    "\n",
    "print(stats.linregress(gb1.year,gb1.job_mass),'\\n')\n",
    "sns.regplot(data=gb1,x='year',y='job_mass',scatter=True)\n",
    "\n",
    "gb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0c81d-459c-49b2-8904-dd3e7e686306",
   "metadata": {},
   "source": [
    "### Stats around the number of fracks by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec73e6-9e91-4808-a8ee-cf874a9bfab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['year'] = df.date.dt.year\n",
    "gb = df.groupby(['year','UploadKey'],as_index=False).size()\n",
    "gb1 = gb.groupby('year',as_index=False)['UploadKey'].count().rename({'UploadKey':'disclosure_cnt'},axis=1)\n",
    "gb1.plot('year','disclosure_cnt',#ylim=(0,16500000),\n",
    "         title='Number of fracking jobs reported by year')\n",
    "ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "gb1['rounded'] = gb1.disclosure_cnt.map(lambda x: round_sig(x,3))\n",
    "\n",
    "print(stats.linregress(gb1.year,gb1.disclosure_cnt),'\\n')\n",
    "sns.regplot(data=gb1,x='year',y='disclosure_cnt',scatter=True)\n",
    "\n",
    "gb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468ad1f-f631-491e-bc78-6ebbf219063d",
   "metadata": {},
   "source": [
    "## Multiple regression\n",
    "First make the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e807a-fe50-49a8-a11f-09f566e67c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.date.dt.year\n",
    "\n",
    "##### independent variables\n",
    "# total number of fracking jobs\n",
    "gb = df.groupby(['year','UploadKey'],as_index=False).size()\n",
    "numjobsgb = gb.groupby('year',as_index=False)['UploadKey'].count().rename({'UploadKey':'disclosure_cnt'},axis=1)\n",
    "\n",
    "# TBWV\n",
    "gb = df.groupby(['year','UploadKey'],as_index=False)['TotalBaseWaterVolume'].first()\n",
    "tbwv_gb = gb.groupby('year',as_index=False)['TotalBaseWaterVolume'].median()\n",
    "\n",
    "# percent contribution of (non-proppant) proprietary percentage to total fracking job\n",
    "c1 =  ~df.IngredientName.isin(propp_lst)\n",
    "c2 = df.bgCAS=='proprietary'\n",
    "gbp = df[c1&c2].groupby(['UploadKey','year'],as_index=False)['PercentHFJob'].sum()\n",
    "gball = df.groupby(['UploadKey','year'],as_index=False)['PercentHFJob'].sum().rename({'PercentHFJob':'totHFJob'},axis=1)\n",
    "mg = pd.merge(gball,gbp,on=['UploadKey','year'],how='left')\n",
    "mg.PercentHFJob.fillna(0,inplace=True)\n",
    "# filter out the bad\n",
    "c1 = mg.totHFJob>95\n",
    "c2 = mg.totHFJob<105\n",
    "mg = mg[c1&c2]\n",
    "percHFJgb = mg.groupby('year', as_index=False)['PercentHFJob'].mean()\n",
    "\n",
    "# perent of disclosures that have at least one proprietary\n",
    "df['is_proprietary'] = df.bgCAS=='proprietary'\n",
    "gb = df.groupby(['UploadKey','year'],as_index=False)[['is_proprietary','ingKeyPresent']].any()\n",
    "gb1 = gb.groupby('year',as_index=False)[['is_proprietary','ingKeyPresent']].sum()\n",
    "gb1['percWithProprietary'] = gb1.is_proprietary/gb1.ingKeyPresent *100\n",
    "percNonZerogb = gb1.drop(['is_proprietary','ingKeyPresent'],axis=1)\n",
    "\n",
    "######## dependent variable - median or mean??\n",
    "nonproppantMass = nonproppantMass[['year','mean']].rename({'mean':'npMass'},axis=1)\n",
    "\n",
    "print(len(numjobsgb),len(tbwv_gb),len(percHFJgb),len(percNonZerogb),len(nonproppantMass))\n",
    "# merge them all\n",
    "mg = pd.merge(numjobsgb,tbwv_gb,on='year')\n",
    "mg = mg.merge(percHFJgb,on='year')\n",
    "mg = mg.merge(percNonZerogb,on='year')\n",
    "mg = mg.merge(nonproppantMass,on='year')\n",
    "\n",
    "mg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5a71b-9cae-41e0-a097-dc18cf1328ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import statsmodels.formula.api as smf \n",
    "# # formula: response ~ predictor + predictor \n",
    "# est = smf.ols(formula='npMass ~ disclosure_cnt + TotalBaseWaterVolume + PercentHFJob + percWithProprietary', data=mg).fit()\n",
    "# est.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
