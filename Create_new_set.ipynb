{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3823ba-1c6f-4b87-849a-8ca8bdc68e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "| <div> <img src=\"https://storage.googleapis.com/open-ff-common/openFF_logo.png\" width=\"100\"/></div>|      |<h1>Adding New Disclosures to the Open-FF Data:<br><br>Download, Curate, Assemble, Test, and Archive<br></h1>|\n",
    "|---|---|---|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3136e-29a8-4c02-8863-739c4f1336cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble code\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from IPython.display import Markdown as md\n",
    "import requests\n",
    "import datetime\n",
    "# itables allow interactive tables but also require downloading html to view\n",
    "use_itables = True\n",
    "\n",
    "if use_itables:\n",
    "    from itables import init_notebook_mode\n",
    "    init_notebook_mode(all_interactive=True)\n",
    "    from itables import show as iShow\n",
    "    import itables.options as opt\n",
    "else:\n",
    "    def iShow(df,maxBytes=0,classes=None):\n",
    "        display(df)\n",
    "\n",
    "lst_str_col = ['APINumber','bgCAS','api10','IngredientName','CASNumber']\n",
    "       \n",
    "def save_df(df,fn):\n",
    "    df.to_parquet(fn)\n",
    "    \n",
    "def get_df(fn):\n",
    "    if fn[-4:]=='.csv':\n",
    "        return pd.read_csv(fn,encoding='utf-8')\n",
    "    return pd.read_parquet(fn)\n",
    "\n",
    "def store_csv(df,fn):\n",
    "    t = df.copy()\n",
    "    for col in lst_str_col:\n",
    "        if col in t.columns:\n",
    "            print(col)\n",
    "            t[col] = \"'\"+t[col]\n",
    "    t.to_csv(fn,encoding='utf-8')    \n",
    "        \n",
    "def clr_cell(txt='Cell Completed', color = '#cfc'):\n",
    "    t = datetime.datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    s = f\"\"\"<div style=\"background-color: {color}; padding: 10px; border: 1px solid green;\">\"\"\"\n",
    "    s+= f'<h3> {txt} </h3> {t}'\n",
    "    s+= \"</div>\"\n",
    "    display(md(s))\n",
    "\n",
    "def completed(status=True,txt=''):\n",
    "    if txt =='':\n",
    "        if status:\n",
    "            txt = 'This step completed normally.'\n",
    "        else:\n",
    "            txt ='Problems encountered in this cell! Resolve before continuing.' \n",
    "    if status:\n",
    "        clr_cell(txt)\n",
    "    else:\n",
    "        clr_cell(txt,color='pink')\n",
    "\n",
    "def get_raw_df():\n",
    "    try: # try to use fresh version\n",
    "        print(f'Using fresh raw_df: {len(raw_df)}')\n",
    "        return raw_df\n",
    "    except:\n",
    "        print('Fetching pickled raw_df')\n",
    "        return pd.read_pickle(os.path.join(work_dir,'raw_flat.pkl'))\n",
    "    \n",
    "def exit_early():\n",
    "    assert False, 'Early Exit triggered!'\n",
    "    \n",
    "    \n",
    "\n",
    "root_dir = ''\n",
    "source_dir = os.path.join(root_dir,'sources')\n",
    "orig_dir = os.path.join(root_dir,'orig_dir')\n",
    "work_dir = os.path.join(root_dir,'work_dir')\n",
    "final_dir = os.path.join(root_dir,'final')\n",
    "code_dir = os.path.join(root_dir,'code')\n",
    "ext_dir = os.path.join(root_dir,'ext')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7469d-b62a-40c5-9e2d-75845620b5b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up\n",
    "Construct a workspace and collect the resources needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757b0e7-dcad-4016-976a-820aa7a5ad84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create directories and fetch previous repository as a reference\n",
    "Before we start downloading new FracFocus data, we set up a working directory structure and collect the resources we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3d2c5-f66f-4613-9909-f33130ae18e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Directories constructed**\n",
    "| directory name | description |\n",
    "| ---: | :--- |\n",
    "|**sources**|founding repo, older archives|\n",
    "|**orig_dir**| expanded zip files, downloaded external files, etc: files used as a model for the next round, but not to be directly saved|\n",
    "|**work_dir**| This is the working directory where new curation files created by these routines are kept. These 'generated' files are saved at the end of the process into either the repository or other archives.|\n",
    "|**ext**| non-FracFocus data files used in constructing the Open-FF data set |\n",
    "|**code**| python files used in this processes.  (User jupyter files, such as this one, are in the root directory.) |\n",
    "|**final**| the place for final files, archives and repositories. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29128ef1-33b5-4f2a-bf46-98bcbc32d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control download: typically set to True\n",
    "#    set to False if you can skip the downloading part of the repo and the external data, for example, during testing.\n",
    "\n",
    "download_repo = False\n",
    "download_ext = False\n",
    "download_FF = False\n",
    "create_raw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28d1d7-6123-45d6-bcbb-91378203672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "\n",
    "\n",
    "dirs = [source_dir,orig_dir,work_dir,code_dir,final_dir,ext_dir]\n",
    "for d in dirs:\n",
    "    if os.path.isdir(d):\n",
    "        print(f'Directory exists: {d}')\n",
    "    else:\n",
    "        print(f'Creating directory: {d}')\n",
    "        os.mkdir(d)\n",
    "    if d==final_dir:\n",
    "        others = ['pickles','curation_files','CAS_ref_files','CompTox_ref_files']\n",
    "        for oth in others:   \n",
    "            subdir = os.path.join(d,oth)\n",
    "            if os.path.isdir(os.path.join(subdir)):\n",
    "                print(f'Directory exists: {subdir}')\n",
    "            else:\n",
    "                print(f'Creating directory: {subdir}')\n",
    "                os.mkdir(subdir)\n",
    "    if d==work_dir:\n",
    "        others = ['new_CAS_REF','new_COMPTOX_REF']\n",
    "        for oth in others:   \n",
    "            subdir = os.path.join(d,oth)\n",
    "            if os.path.isdir(os.path.join(subdir)):\n",
    "                print(f'Directory exists: {subdir}')\n",
    "            else:\n",
    "                print(f'Creating directory: {subdir}')\n",
    "                os.mkdir(subdir)\n",
    "\n",
    "s_repo_name = os.path.join(source_dir,'cloud_repo.zip')\n",
    "\n",
    "if download_repo:\n",
    "    try:\n",
    "        print(\"This step may take a few minutes...\")\n",
    "        # url = 'https://storage.googleapis.com/open-ff-common/repos/testRepo.zip'\n",
    "        url = 'https://storage.googleapis.com/open-ff-common/repos/cloud_repo.zip'\n",
    "        print(f'Downloading repository from {url}')\n",
    "        r = requests.get(url, allow_redirects=True,timeout=20.0)\n",
    "        open(s_repo_name, 'wb').write(r.content)\n",
    "        # Now expand repo into both orig_dir\n",
    "    except:\n",
    "        completed(False,'Problem downloading repository!')\n",
    "else:\n",
    "    print('Continuing without downloading fresh copy of repository')\n",
    "print(' -- Unpacking existing repository into \"orig\" directory')\n",
    "shutil.unpack_archive(s_repo_name,orig_dir)\n",
    "completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996c830-b6b5-43e5-b9d8-0dec6d803364",
   "metadata": {},
   "source": [
    "## Download external files used to assemble final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860db2a3-5e5e-4f63-945e-9e15d1d04183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "\n",
    "ext_name = os.path.join(ext_dir,'openff_ext_files.zip')\n",
    "if download_ext:\n",
    "    try:\n",
    "        print(\"This step may take several minutes. There are big files to transfer...\")\n",
    "        url = 'https://storage.googleapis.com/open-ff-common/openff_ext_files.zip'\n",
    "        print(f'Downloading external files from {url}')\n",
    "        r = requests.get(url, allow_redirects=True,timeout=20.0)\n",
    "        open(ext_name, 'wb').write(r.content)\n",
    "        # Now expand into ext dir\n",
    "        print('Unpacking zip into \"ext\" directory')\n",
    "        shutil.unpack_archive(ext_name,ext_dir)\n",
    "        completed()\n",
    "    except:\n",
    "        completed(False,'Problem downloading external files!')\n",
    "else:\n",
    "    completed(True,'Completed without new external download')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a3934-b10c-4962-aa20-2af70b95827f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download raw files from FracFocus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590894b-8e3c-4d9d-8813-e92231a0bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "\n",
    "import core.fetch_new_bulk_data as fnbd\n",
    "if download_FF:\n",
    "    completed(fnbd.store_FF_bulk(newdir = work_dir,sources=source_dir, archive=True, warn=True))\n",
    "else:\n",
    "    completed(True,'Completed using saved FF download')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77dd38-613a-468d-9e1f-62291a33552a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create raw FracFocus set in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ce504-20ca-4948-a431-8b52353c65ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#exit_early() # uncomment this line to stop execution at this cell.\n",
    "import core.Bulk_data_reader as bdr\n",
    "if create_raw:\n",
    "    rff = bdr.Read_FF(in_name='testData.zip', \n",
    "                      zipdir=work_dir,workdir = work_dir,\n",
    "                      origdir=orig_dir,\n",
    "                      flat_pickle = 'raw_flat.pkl')\n",
    "    raw_df = rff.import_raw()\n",
    "    # get number of records from old, repository data set\n",
    "    oldrecs = pd.read_pickle(os.path.join(orig_dir,'pickles','chemrecs.pkl'))\n",
    "    if len(oldrecs)>len(raw_df):\n",
    "        completed(False,'The old repository has MORE records than current download. Bad download??')\n",
    "    else:\n",
    "        completed(len(raw_df)>0)\n",
    "else:\n",
    "    raw_df = get_raw_df()\n",
    "    completed(True,'Completed without new FF download')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b652564-05ea-4908-b443-61b8559befd3",
   "metadata": {},
   "source": [
    "#### Add new disclosures to UploadKey file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760afd9-6a8c-4efc-b8a2-20613036d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.today()\n",
    "datefn= os.path.join(orig_dir,'curation_files','upload_dates.csv')\n",
    "outdf = pd.read_csv(datefn)\n",
    "uklst = outdf.UploadKey.unique()\n",
    "\n",
    "df = get_raw_df()\n",
    "ndf = df[~df.UploadKey.isin(uklst)].copy() # just the new ones\n",
    "\n",
    "gb = ndf.groupby('UploadKey',as_index=False)['OperatorName'].count()\n",
    "gb['date_added'] = today.strftime(\"%Y-%m-%d\")\n",
    "gb.rename({'OperatorName':'num_records'}, inplace=True,axis=1)\n",
    "print(f'Number of new disclosures added to list: {len(gb)}')\n",
    "outdf = pd.concat([outdf,gb],sort=True)\n",
    "outdf.to_csv(os.path.join(work_dir,'upload_dates.csv'),index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b980b90-c3ea-450f-88d0-e77092e14f32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Curation steps\n",
    "These steps are a mix of automated and hand-performed curation tasks. The hand performed tasks require the user to examine database values in spreadsheets and make decisions on those values about individual records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80081f-4ee8-43e1-b948-fbfee984affb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `CASNumber` and `IngredientName` curation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a07cdb-6364-4ae4-8f40-4dcbf68a9fb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Open-FF uses both raw input fields `CASNumber` and `IngredientName` to clarify chemical identity in each record.  These two fields **should** agree on the identity, but often only one field provides unambiguous identification (usually `CASNumber`) and sometimes the two are conflicting.  Our target is an accurate `bgCAS`, which is our \"best guess\" at a CAS Registration Number for the material reported in the disclosure.\n",
    "That is,\n",
    "> Unique `CASNumber` | `IngredientName` pair  $\\rightarrow$ `bgCAS`\n",
    "\n",
    "There are currently over 28,000 unique pairs.\n",
    "\n",
    "The curation process outlined below gets our identification as close as possible to our target.  It requires using several sources of information and part of the process includes collecting that information.  Some steps are partially automated whereas other steps require our judgement and are therefore manual.  \n",
    "\n",
    "This process is also incremental - we only need to curate the *new* chemical identifiers in the most recent download.  However, this process can also be used to examine the whole curated set to refine identification performed earlier.  \n",
    "\n",
    "Resources needed to create the CAS-Ing list:\n",
    "- CAS_curated: a list of `CASNumber` values and the tentative `bgCAS` number they imply.\n",
    "- `IngredientName` synonym list: list of synonyms (and associated CAS number) to weigh against `IngredientName`. This is created from a collection of CAS and CompTox references.\n",
    "- `TradeName` values associated with CAS-Ing pairs -- this aspect is still in development, though curators may manually examine TradeNames to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6780c49-8b7e-49ea-98d2-2b4b77591ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### step A - use previous list to find any new `CASNumber` values\n",
    "1. compare list of `CASNumber` values in rawdf to list of `CASNumber` in *olddir/curation_files/cas_curated.csv*\n",
    "1. make and display list of those new ones.\n",
    "\n",
    "#### Next steps for YOU:\n",
    ">Next steps for **new** `CASNumber` (see note below): \n",
    ">- if the implied chemical is not in the CAS references, go to SciFinder and make new entry (manual!)\n",
    ">- otherwise, can skip the SciFinder steps, but go to the CAS_curate step. \n",
    ">\n",
    ">If there are no **new** `CASNumber`, \n",
    ">- skip all the way to the moving the current CAS_curate.csv to *newdir/curation_files/CAS_curate.csv*\n",
    "\n",
    "Note: these \"new\" `CASNumber` values can be completely new chemicals or just a new version of an already used material (for example, we might find '00000050-00-0' for the authoritative CASRN '50-00-0' that is already documented in Open-FF.).   They may also be something that will not resolve into a valid CASRN, for example: 'proprietary by operator'. You will assign appropriate 'bgCAS' values in the curation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236affb-177d-46f1-8b1e-2a4b3c4cc244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP A\n",
    "# exit_early() # uncomment this line to stop execution at this  cell.\n",
    "import core.process_CAS_ref_files as cref\n",
    "import builder_tasks.CAS_master_list as casmaster\n",
    "\n",
    "cref.compile_all_refs(indir = orig_dir,outdir = work_dir)\n",
    "\n",
    "newcas = casmaster.get_new_tentative_CAS_list(get_raw_df(),orig_dir=orig_dir,work_dir=work_dir)\n",
    "if len(newcas)>0:\n",
    "    iShow(newcas)\n",
    "    if len(newcas[newcas.tent_is_in_ref==False])>0:\n",
    "        display(md('## Go to STEP B: Use SciFinder for `CASNumber`s not in reference already'))\n",
    "    else:\n",
    "        display(md('## Nothing to look up in SciFinder, but some curation necessary.  Skip to **Step XX**'))\n",
    "else:\n",
    "    display(md('### No new CAS numbers to curate.  Skip to **Step XX**'))\n",
    "completed() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb345e-576a-4067-b4cf-7bc190803ada",
   "metadata": {},
   "source": [
    "### Step B - Add chem info from SciFinder of new tentative CAS numbers into CAS_ref files\n",
    "If a chemical on this list hasn't been seen before in FF, we need to add some information into the CAS_ref files.\n",
    "To do that, we currently use SciFinder, which is a product of the Chemical Abstract Service, the naming authority for materials.\n",
    "\n",
    "Save the file in `new_CAS_ref` directory of **work_dir** \n",
    "\n",
    "Once you have saved the file, run the following cell to **verify** that the new `CASNumber`s that you've found to be valid and created a reference for, actually made it into the SciFinder reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6c70e-43ba-4c86-b2b6-2043f90c4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step B\n",
    "import core.process_CAS_ref_files as cref\n",
    "cref.compile_all_refs(indir = orig_dir,outdir = work_dir, newdir = os.path.join(work_dir,'new_CAS_ref'))\n",
    "\n",
    "newcas = casmaster.get_new_tentative_CAS_list(get_raw_df(),orig_dir=orig_dir,work_dir=work_dir)\n",
    "casmaster.make_CAS_to_curate_file(newcas,ref_dir=orig_dir,work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5893aa1-9817-4c49-93bb-01efda6722c5",
   "metadata": {},
   "source": [
    "### Step C - Curate the CAS_to_curate file\n",
    "In this step you will manually edit the *new_dir/CAS_to_curate.csv* file to curate the new `CASNumber` values. There are typically only a handful of lines in this file that you need to curate, just those newly discovered in the latest FF download. \n",
    "\n",
    "Your task is to XXXXXXXXXX\n",
    "\n",
    "Once you have completed the editing, save the file back to *new_dir/CAS_curated.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d406ed-0fe7-4e4a-8d4f-2f9fa52561d8",
   "metadata": {},
   "source": [
    "#### Step C.1 - Make sure all `CASNumber` values have been curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e63eb-1a8a-4df7-8c5e-e2cede407f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "\n",
    "flag = casmaster.is_new_complete(work_dir)\n",
    "if flag:\n",
    "    completed()\n",
    "else:\n",
    "    completed(False,\"More CASNumbers remain to be curated. Don't proceed until corrected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f60370-c8bc-4a5a-a9f1-416490938582",
   "metadata": {},
   "source": [
    "---\n",
    "### Step D - Update CompTox data\n",
    "The metadata from EPA's CompTox system informs OpenFF in a number of ways. Now that we have a full CAS list, we need to update the CompTox data. \n",
    "**If no new CAS numbers were added to CAS_curated.csv, you can skip this step and jump to the CAS | Ing processing**\n",
    "\n",
    "1. Open [CompTox batch search](https://comptox.epa.gov/dashboard/batch-search)\n",
    "1. Under \"Select Input Type(s)\", check \"CASRN\"\n",
    "1. Open the  *work_dir/comptox_search_list.csv* file in something like Excel or OpenOffice.\n",
    "1. Copy and paste all CAS numbers in the `curatedCAS` column into the CompTox webpage \"Enter Identifiers to Search\" box.  You can skip the non_CAS numbers like 'proprietary'. They mean nothing to CompTox.\n",
    "1. On the Comptox page, click the \"Choose Export Options\" button.\n",
    "1. Under \"Choose Export Format,\" select \"Excel.\"\n",
    "1. In the \"Chemical Identifiers\" section, make sure that the following are checked:\n",
    "- Chemical Name\n",
    "- CAS RN\n",
    "- DTXSID\n",
    "- IUPAC Name\n",
    "1. Under \"Enhanced Data Sheets,\" select \"Synonyms and Identifiers\"\n",
    "1. Finally, click \"Download Export File\". This can take several minutes, or even stall if EPA's servers have heavy use.\n",
    "1. Once the file has been downloaded to your machine, RENAME it \"comptox_batch_results.xlsx\" (don't open it, just rename it!) **NOTE that recently, this process does not complete but hangs indefinitely.**  The current work-around is to \n",
    "    - Deselect the \"Synonyms and Identifiers\" checkbox and click \"Download Export File\" again.\n",
    "    - Download the file, rename it as \"comptox_batch_results_no_syn.xlsx\"\n",
    "1. Move that file to *work_dir*\n",
    "1. Run the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4c9e8-ebf2-49c4-9781-7d7f7e4ce693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.process_comptox_batch as pcb\n",
    "ctfn = os.path.join(work_dir,'comptox_batch_results.xlsx')\n",
    "if not os.path.exists(ctfn):\n",
    "    completed(False,'CompTox batch results file not found')\n",
    "else:\n",
    "    pcb.build_new_comptox_refs(work_dir)\n",
    "    completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a3fd6-ff98-4d29-9c97-35fa76c4ab9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Start CAS|Ing processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d0397-53c5-4c6e-a5ec-ae86f4cb4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "import builder_tasks.CAS_2_build_casing as cas2\n",
    "import builder_tasks.IngName_curator as IngNc\n",
    "new_casing = cas2.make_casing(get_raw_df(),ref_dir=orig_dir,work_dir=work_dir) \n",
    "t = new_casing[new_casing.first_date.isna()].copy()\n",
    "if len(t)>0:\n",
    "    refdic = IngNc.build_refdic(ref_dir=work_dir)\n",
    "    refdic = IngNc.summarize_refs(refdic)\n",
    "    fsdf = IngNc.full_scan(t,refdic,pd.DataFrame(),work_dir)\n",
    "    # print(fsdf.columns)\n",
    "    fsdf = IngNc.analyze_fullscan(fsdf)\n",
    "    # print(fsdf.columns)\n",
    "    fsdf.to_csv(os.path.join(work_dir,'casing_TO_CURATE.csv'),quotechar='$',encoding='utf-8')\n",
    "    fsdf = fsdf.reset_index()\n",
    "    iShow(fsdf[['CASNumber', 'curatedCAS', 'IngredientName', 'recog_syn', 'synCAS',\n",
    "           'match_ratio', 'n_close_match', 'source', 'bgCAS', 'rrank', 'picked']],\n",
    "          maxBytes=0,classes=\"display compact cell-border\")\n",
    "    completed()\n",
    "else:\n",
    "    # if no new, copy original casing_curated.csv to work_dir\n",
    "    shutil.copy(os.path.join(orig_dir,'curation_files','casing_curated.csv'),work_dir)\n",
    "    completed(True,'No new CAS|Ing to process; skip next step')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e40d3f-b82a-46a6-8331-fda2cd9f9e9c",
   "metadata": {},
   "source": [
    "### step 9 - curate the new CAS|Ing pairs \n",
    "\n",
    "- make any desired changes to casing_TO_CURATE.csv.\n",
    "- save file as *casing_modified.csv* in **newdir**\n",
    "- then run the following code.  This step will keep only those lines where 'picked'=='xxx' and it will add today's date as the first seen date.\n",
    "\n",
    "It will then add these lines to the master casing_curated file that will be used in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245ab99-deb9-4580-ba2c-3933e3dc6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "try:\n",
    "    modified = pd.read_csv(os.path.join(work_dir,'casing_modified.csv'),quotechar='$',encoding='utf-8')\n",
    "except:\n",
    "    assert False, \"casing_modified.csv not found.  Did you mean to skip this step?\"\n",
    "modified['first_date'] = 'D:'+f'{Today}'\n",
    "# print(modified.columns)\n",
    "oldcasing = pd.read_csv(os.path.join(orig_dir,'curation_files','casing_curated.csv'),quotechar='$',encoding='utf-8')\n",
    "try: # works only on casing gerenated in non-cloud env. \n",
    "    oldcasing['synCAS'] = oldcasing.prospect_CAS_fromIng\n",
    "    oldcasing['source'] = oldcasing.bgSource\n",
    "except:\n",
    "    pass\n",
    "together = pd.concat([modified[modified.picked=='xxx'][['CASNumber','IngredientName','curatedCAS','recog_syn','synCAS',\n",
    "                                                        'bgCAS','source','first_date','n_close_match']],\n",
    "                      oldcasing[['CASNumber','IngredientName','curatedCAS','recog_syn','synCAS','bgCAS','source',\n",
    "                                  'first_date','change_date','change_comment']] ],sort=True)\n",
    "together = together[['CASNumber','IngredientName','curatedCAS','recog_syn','synCAS','n_close_match',\n",
    "                                                        'bgCAS','source','first_date','change_date','change_comment']]\n",
    "together.to_csv(os.path.join(work_dir,'casing_curated.csv'),quotechar='$',encoding='utf-8')\n",
    "completed()\n",
    "iShow(together,maxBytes=0,classes=\"display compact cell-border\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54564d48-c915-4e8a-8a68-1b784cf5d7e2",
   "metadata": {},
   "source": [
    "### step 10 - Verify that all CAS/Ing pairs are curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27aab4b-5448-42a0-8ddf-3d650c954688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "import builder_tasks.CAS_2_build_casing as cas2\n",
    "completed(cas2.is_casing_complete(get_raw_df(),work_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb49d7e-5e4d-4e18-8107-43d8d1d4e6cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Company Name curation tasks\n",
    "\n",
    "The company names used in FracFocus are not standardized; searching for all records of a company using the raw FracFocus data can be a tedious and frustrating task.   Open-FF uses a translation table to take raw company names (`OperatorName` and `Supplier`) and cluster them into categories that refer to the same company.  \n",
    "\n",
    "The cells below first finds new company names that need curation attention and stores them in a file called *company_xlateNEW.csv*.  Typically, for about 1000 new disclosures, there are about 50 new names to curate, with many being slight variations on already curated names or brand new companies.  The users job is to do that curation (it usually takes just a few minutes).  The user saves that curated file and that will be used to build a new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd680c2-1ac6-4bbe-b736-d7b64a936db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "import builder_tasks.CompanyNames_make_list as complist\n",
    "\n",
    "companies = complist.add_new_to_Xlate(get_raw_df(),ref_dir=orig_dir,out_dir=work_dir)\n",
    "\n",
    "completed()\n",
    "iShow(companies.reset_index(drop=True),maxBytes=0,columnDefs=[{\"width\": \"100px\", \"targets\": 0}],\n",
    "     classes=\"display compact cell-border\", scrollX=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71eb26-a7da-4f34-bbe8-13b57590c087",
   "metadata": {},
   "source": [
    "### Now curate the new company names\n",
    "Edit the *company_xlateNEW.csv* file so that `xlateName` is acceptable, the `first_date` is filled out, and the `status` is set to **curated**. \n",
    "\n",
    "Save those changes as *work_dir/company_xlate.csv*. \n",
    "\n",
    "Run the following cell and verify that you have no company names to curate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f7c37-4960-4c2d-aeab-8c15ab3ab8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "import builder_tasks.CompanyNames_make_list as complist\n",
    "completed(complist.is_company_complete(work_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b880aba-e33e-4542-97ec-3016014d0d86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Location curation tasks\n",
    "\n",
    "Like the other text fields in FracFocus, state and county names are not required to be standardized.  We try to create curated, standardized versions where we can to help with location errors detection.  Typically, very few new locations are added and so curation is often not even required with this data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21d4a3-34f3-4ced-85be-42d17607424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "\n",
    "import builder_tasks.Location_cleanup as loc_clean\n",
    "locobj = loc_clean.Location_ID(get_raw_df(),ref_dir=orig_dir,out_dir=work_dir,ext_dir=ext_dir)\n",
    "newloc = locobj.clean_location()\n",
    "completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bd036-a8ea-4a88-a01c-6e7db09f0d60",
   "metadata": {},
   "source": [
    "### Curate results\n",
    "**If there are new locations**, curate the *work_dir/location_curatedNEW.csv* file and save to *work_dir/location_curated.csv*\n",
    "\n",
    "Then run the location check again to make sure you curated all the new locations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7d71e-6ed6-48d3-91f8-60eaa2422563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builder_tasks.Location_cleanup as loc_clean\n",
    "locobj = loc_clean.Location_ID(get_raw_df(),ref_dir=work_dir,out_dir=work_dir)\n",
    "completed(locobj.is_location_complete())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9903b2-3d6e-4821-9278-cf285b8377a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Water carrier detection\n",
    "To perform accurate calculation of mass, it is critical that the water carrier records in disclosures are identified.\n",
    "\n",
    "In this current version of Open-FF, all water carrier determinations are performed with code.  No hand-curation is used. We came to the conclusion that, in the irregular disclosures that would be a target for hand curation, there are too many moving parts to make consistent decisions over the whole set especially with new disclosures being added all the time.  By using only coded algorithms to detect the carriers, we can apply consistant rules over the entire set.   \n",
    "\n",
    "The current set of algorithms reject about 54,000 disclosures are being clearly ineligible for carrier detection (43,000 simply because they lack ingredient data).  Of the remaining 150,000, about 1% are not caught by the detection algorithms.  Data on those are available in a saved file here for user examination. While calculated masses will not be performed on that small set, `MassIngredient` may still be explored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d336c0f-1ac6-4482-8b71-e17260ed819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "import builder_tasks.Carrier_1_identify_in_new as car1\n",
    "\n",
    "carobj = car1.Carrier_ID(get_raw_df(),ref_dir=orig_dir,out_dir=work_dir)\n",
    "completed(carobj.create_full_carrier_set())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d390778-fb9c-44ec-805e-b9d6bde10040",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build and save Open-FF data set\n",
    "\n",
    "**Start these steps only after all curation steps have been completed successfully!**\n",
    "\n",
    "This step takes all of the files created in the curation steps and applies them together to the raw data.  Additionally, hooks to external data sources are used to create fields that better identify chemicals, locations etc.  The result of this step is a set of tables that can be used to further build a flat data set (such as a CSV file) or even a relational database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc7229-b77e-4df7-bc55-417059bcd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move all relevant files to the final directory for the Build step\n",
    "# # \n",
    "# lst = os.listdir(os.path.join(orig_dir,'curation_files'))\n",
    "# lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bc31f-d464-4e5b-b68a-957486d02735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the CAS and CompTox ref files\n",
    "cdir = os.path.join(orig_dir,'CAS_ref_files')\n",
    "fdir = os.path.join(final_dir,\"CAS_ref_files\")\n",
    "shutil.copytree(src=cdir,dst=fdir,dirs_exist_ok=True)\n",
    "\n",
    "cdir = os.path.join(orig_dir,'CompTox_ref_files')\n",
    "fdir = os.path.join(final_dir,\"CompTox_ref_files\")\n",
    "shutil.copytree(src=cdir,dst=fdir,dirs_exist_ok=True)\n",
    "\n",
    "cdir = os.path.join(work_dir,'new_CAS_REF')\n",
    "fdir = os.path.join(final_dir,\"CAS_ref_files\")\n",
    "shutil.copytree(src=cdir,dst=fdir,dirs_exist_ok=True)\n",
    "\n",
    "# get files from orig_dir\n",
    "files = [\n",
    " 'carrier_list_curated.csv',\n",
    " 'missing_values.csv',\n",
    " 'new_state_county_ref.csv']\n",
    "for fn in files:\n",
    "    shutil.copy(os.path.join(orig_dir,'curation_files',fn),\n",
    "                os.path.join(final_dir,'curation_files',fn))\n",
    "# get the curation files from the working dir\n",
    "files = ['carrier_list_auto.csv',\n",
    " 'carrier_list_prob.csv',\n",
    " 'casing_curated.csv',\n",
    " 'CAS_curated.csv',\n",
    " 'CAS_ref_and_names.csv',\n",
    " 'CAS_synonyms.csv',\n",
    " 'CAS_synonyms_CompTox.csv',\n",
    " 'company_xlate.csv',\n",
    " 'IngName_non-specific_list.csv',\n",
    " 'location_curated.csv',\n",
    " 'uploadKey_ref.csv', 'upload_dates.csv']\n",
    "for fn in files:\n",
    "    shutil.copy(os.path.join(work_dir,fn),\n",
    "                os.path.join(final_dir,'curation_files',fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627555cd-e3b3-400c-b388-f8d7dc8b02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exit_early() # uncomment this line to stop execution at this cell.\n",
    "import core.Data_set_constructor as dsc\n",
    "\n",
    "dataobj = dsc.Data_set_constructor(get_raw_df(),final_dir,final_dir,ext_dir)\n",
    "out = dataobj.create_full_set()\n",
    "completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d497ba-9232-4ce1-9b32-72ef57bdd1fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create flat data set and test it\n",
    "This step uses the set of tables created earlier to build a single 'flat' data file as well as to run some basic tests on the new data set.  Note that because the full data set is very large (too big for excel) and CSV files are cumbersome at this size, we are using the **parquet** format which is much faster and takes up far less space.  To create an equivalent CSV file, see this XXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917146a8-a5a3-486d-a2b5-6d18be6cda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit_early() # uncomment this line to stop execution at this cell.\n",
    "\n",
    "import core.Analysis_set as a_set\n",
    "ana_set = a_set.Full_set(sources=final_dir,outdir=final_dir)\n",
    "df = ana_set.get_set(verbose=False)\n",
    "completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a19a79-d245-4b50-8b84-db14726c0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform the tests\n",
    "import core.Tests_of_final as tof\n",
    "tests = tof.final_test(df)\n",
    "tests.run_all_tests()\n",
    "completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36592e98-d8f8-4a58-99e4-d2d8eda18883",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make repository\n",
    "One the data set has been created, saved and tested, we construct a \"repository.\"  Once created, this repository is intended to be **read only**, that is, no changes should be made to it.  The idea is that when using a given repository, analysts can depend on it being frozen in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2318dd-421b-4b5f-a82b-a63228275b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core.make_data_repo as mdr\n",
    "import shutil\n",
    "repo_name = 'cloud_repo'\n",
    "repodir = os.path.join(final_dir,repo_name)\n",
    "pklsource = os.path.join(final_dir,'pickles')\n",
    "\n",
    "try:\n",
    "    os.mkdir(repodir)\n",
    "except:\n",
    "    print(f'{repodir} already exists?')\n",
    "pickledir = os.path.join(repodir,'pickles')\n",
    "try:\n",
    "    os.mkdir(pickledir)\n",
    "except:\n",
    "    print(f'{pickledir} already exists?')\n",
    "curdir = os.path.join(repodir,'curation_files')\n",
    "try:\n",
    "    os.mkdir(curdir)\n",
    "except:\n",
    "    print(f'{curdir} already exists?')\n",
    "        \n",
    "# copy CAS and CompTox reference files\n",
    "cdir = os.path.join(repodir,'CAS_ref_files')\n",
    "sdir = os.path.join(final_dir,\"CAS_ref_files\")\n",
    "shutil.copytree(sdir,cdir,dirs_exist_ok=True)\n",
    "\n",
    "cdir = os.path.join(repodir,'CompTox_ref_files')\n",
    "sdir = os.path.join(final_dir,\"CompTox_ref_files\")\n",
    "shutil.copytree(sdir,cdir,dirs_exist_ok=True)\n",
    "\n",
    "# copy curation files\n",
    "cdir = os.path.join(repodir,'curation_files')\n",
    "sdir = os.path.join(final_dir,\"curation_files\")\n",
    "shutil.copytree(sdir,cdir,dirs_exist_ok=True)\n",
    "\n",
    "# copy pickles\n",
    "cdir = os.path.join(repodir,'pickles')\n",
    "sdir = os.path.join(final_dir,\"pickles\")\n",
    "shutil.copytree(sdir,cdir,dirs_exist_ok=True)\n",
    "\n",
    "# Other files to copy\n",
    "shutil.copy(os.path.join(final_dir,'full_df.parquet'),repodir)\n",
    "\n",
    "print('Making archive...')\n",
    "completed(shutil.make_archive(repodir, 'zip', repodir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514e508-bfd0-465b-bcf1-c6ac932775f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
