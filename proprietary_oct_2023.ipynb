{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951c01d8-18ff-4bf8-a705-997068fd5b4a",
   "metadata": {
    "id": "9e3823ba-1c6f-4b87-849a-8ca8bdc68e5d",
    "tags": []
   },
   "source": [
    "<div> <center><img src=\"https://storage.googleapis.com/open-ff-common/openFF_logo.png\" width=\"100\"/></div>\n",
    "<h1><center>Proprietary notebook: version Oct 2023</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3a300-3359-4aa1-b69d-bda64b070ba6",
   "metadata": {
    "id": "af0fd3b5-cdf7-4f5f-9310-0447845d2dbe"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/gwallison/intg_support.git &>/dev/null;\n",
    "!pip install itables  &>/dev/null;\n",
    "!pip install geopandas  &>/dev/null;\n",
    "\n",
    "import urllib\n",
    "urllib.request.urlretrieve('https://storage.googleapis.com/gwa-test/georef-united-states-of-america-county.geojson',\n",
    "                            'counties.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624c93d-35a1-4b11-97ca-dc4e27da779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run intg_support/proprietary_aug_2023.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83b299-e37c-4e4d-8c48-e12905eee009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import gca, mpl\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# get full set of data\n",
    "import intg_support.geo_tools as gt\n",
    "work_dir = ''\n",
    "# work_dir = './tmp'\n",
    "get_fulldf(work_dir=work_dir) # this downloads most recent from the web.\n",
    "df = get_df(os.path.join(work_dir,'full_df.parquet'))\n",
    "df = df[df.in_std_filtered]\n",
    "\n",
    "# NOW HAS FILTER to remove disclosures without chemicals!!\n",
    "df = df[df.ingKeyPresent]\n",
    "\n",
    "# uncomment following line to limit the dates\n",
    "df = df[(df.date.dt.year>2013)&(df.date.dt.year<2023)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d449303-e100-4389-942b-1d73161e8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a proprietary only data set\n",
    "prop_df = df[df.bgCAS=='proprietary'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de39b04-5ba1-4481-9994-96c5df205ca0",
   "metadata": {},
   "source": [
    "# identify as many proprietary proppants as we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe0d51-538f-4e9a-8b69-32daffa70135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"\"\"30/70 Permian\n",
    "aa-400 (aluminum oxide)\n",
    "aluminum oxide\n",
    "amorphous Silica\n",
    "amorphous silica\n",
    "amorphous silicia\n",
    "ceramic microspheres\n",
    "ceramic microspheres/glutaraldehyde\n",
    "ceramic propant\n",
    "ceramic proppant\n",
    "ceramic proppant proprietary\n",
    "copolymer resin fracturing proppant\n",
    "corundum\n",
    "crys4808-60-7talline sio2\n",
    "crystalline cristobalite\n",
    "crystalline silica\n",
    "crystalline silica (quartz)\n",
    "crystalline silica (quartz), proprietary\n",
    "crystalline silica, quartz\n",
    "crystalline silica: cristobalite\n",
    "crystalline silica: quartz (sio2)\n",
    "crystalline silica(quartz),proprietary\n",
    "crystalline sio2\n",
    "fumed silica\n",
    "hydrophobic silica\n",
    "hydrated magnesium silicate\n",
    "magnesium silicate hydrate (talc)\n",
    "meghemite\n",
    "non- crystalline silica (impurity)\n",
    "non-crystalline silica \n",
    "proppant\n",
    "proprietary quartz\n",
    "proprietary silica \n",
    "quartz\n",
    "quartz (sio2)\n",
    "sand\n",
    "silica substrate\n",
    "silica substitute with bonded coatings\n",
    "silicate minerals - ts\n",
    "zinc oxide\"\"\"\n",
    "propp_lst = s.split('\\n')\n",
    "\n",
    "prop_df['is_proppant'] = prop_df.IngredientName.isin(propp_lst)\n",
    "print(f'Total number of identified proprietary proppant records: {prop_df.is_proppant.sum()}')\n",
    "prop_df[prop_df.is_proppant].IngredientName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ef232-7dd9-4084-90c2-39d7cf08bceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove one obvious error point\n",
    "import numpy as np\n",
    "prop_df.calcMass = np.where(prop_df.calcMass>100000000,np.NaN,prop_df.calcMass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007096c8-30e3-47ff-b30e-80d841fd9b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pylab import gca, mpl\n",
    "ax = prop_df[prop_df.is_proppant].plot('date','calcMass',style='o',alpha=.5,legend=False)\n",
    "ax.set_title('Mass of proprietary records with proppant ingredient name')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Mass (lbs.) of individual proprietary records')\n",
    "# prop_df[~prop_df.is_proppant].plot('date','calcMass', style='o',alpha=.2, ax=ax, legend=False)\n",
    "ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb8149-1cf0-4d78-a1cf-9698ae62a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def my_formatter(x, pos):\n",
    "     return \"{}\".format('{x:,.4f}' if x<1 else '{x:,.0f}' )\n",
    "    \n",
    "sns.set_theme(style=\"ticks\")\n",
    "prop_df['logMass'] = np.where(prop_df.calcMass>0.00001,np.log10(prop_df.calcMass),np.NaN) \n",
    "prop_df['year'] = prop_df.date.dt.year\n",
    "# ax = sns.boxplot(data=prop_df[~prop_df.is_proppant], x='year',y='logMass',showfliers=False)\n",
    "ax = sns.boxplot(data=prop_df[(~prop_df.is_proppant)&(prop_df.calcMass>.001)], x='year',y='calcMass',\n",
    "                 showfliers=False, color='skyblue')\n",
    "# sns.regplot(data=prop_df[(~prop_df.is_proppant)&(prop_df.calcMass>.001)], x='year',y='calcMass',\n",
    "#             ax=ax, scatter=False)\n",
    "# # ax.set_title('Mass of non-proppant proprietary records (log(Mass))')\n",
    "ax.set_ylabel('log of calculated mass')\n",
    "plt.yscale(\"log\")\n",
    "ax.tick_params(labelright=True, right=True, which='both')\n",
    "# ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.4f}'))\n",
    "# ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a838d4f-a555-42c4-b994-4b2fea1251de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_df[~prop_df.is_proppant].groupby('year')['calcMass'].agg(['median','mean','sum','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc3839-9e11-48f0-a077-0028e6b798b3",
   "metadata": {},
   "source": [
    "# basic proprietary stats (added 8/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7aaa7-ceb5-4f68-93eb-a6da0f039741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Total calculated mass of proprietary records: {round_sig(prop_df.calcMass.sum(),3)} lbs.')\n",
    "prop_df['added_year'] = prop_df.date_added.dt.year\n",
    "print(f'Calculated mass of proprietary records added in 2023: {round_sig(prop_df[prop_df.added_year==2023].calcMass.sum(),3)} lbs.')\n",
    "\n",
    "prop_df['year'] = prop_df.date.dt.year\n",
    "# with pd.option_context(\"display.float_format\", \"{:,.0f}\".format):\n",
    "gb = prop_df.groupby('year',as_index=False)['calcMass'].sum()\n",
    "gb['rounded'] = gb.calcMass.map(lambda x: round_sig(x,3))\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3561a45-f0c6-4103-94a0-a54835aa2b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_num_disc = df.UploadKey.unique().size\n",
    "num_disc_with_prop = prop_df.UploadKey.unique().size\n",
    "print(f'Total num of disclosures in set:      {total_num_disc:,}')\n",
    "print(f'Total with at least one propriertary: {num_disc_with_prop:,}')\n",
    "print(f'Overall Percent with proprietary:     {num_disc_with_prop/total_num_disc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d346b-1e5e-4a82-a608-a57b5d142151",
   "metadata": {
    "tags": []
   },
   "source": [
    "# show all the Purpose fields\n",
    "You could also look at the [Browser's version](https://storage.googleapis.com/open-ff-browser/proprietary/analysis_proprietary.html#raw) of this.  Extra long purposes (using indicating multiple products) are truncated and combined.  They are not very useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5f131-a0c1-4658-b82a-ab4b9bf10dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prop_df.groupby('Purpose',as_index=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ca8cf-aa54-47fa-9d0e-aab42f87065d",
   "metadata": {},
   "source": [
    "# make graphs of operators and suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb717444-3f0c-48d3-b6b6-dece59b25229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cas = prop_df.bgCAS.iloc[0]\n",
    "# use the most common name given in FF for the label\n",
    "gb1 = prop_df.groupby('bgOperatorName')['OperatorName'].agg(lambda x: x.value_counts().index[0])\n",
    "gb1 = gb1.reset_index()\n",
    "gb1.columns = ['bgOperatorName','op_common']\n",
    "mg = pd.merge(prop_df,gb1,on='bgOperatorName',how='left')\n",
    "newmg = mg.groupby('op_common')['UploadKey'].count().sort_values(ascending=False)\n",
    "ax = newmg[:15].plot.barh(figsize=(7,7))\n",
    "ax.tick_params(axis=\"y\", labelsize=14)\n",
    "ax.tick_params(axis=\"x\", labelsize=14)\n",
    "plt.xlabel('Number of records',fontsize=16);\n",
    "plt.ylabel('Operating Company',fontsize=16);\n",
    "plt.title(f'Number of \"proprietary\" records, by operator',fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73753b35-d7a3-4796-8780-2357d91ec7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_full = df.groupby(['bgOperatorName','UploadKey'],as_index=False).size()\n",
    "gb_full = gb_full.groupby('bgOperatorName')['UploadKey'].count().reset_index()\n",
    "gb_full.columns = ['bgOperatorName','disclosure_cnt_all']\n",
    "\n",
    "gb_prop = prop_df.groupby(['bgOperatorName','UploadKey'],as_index=False).size()\n",
    "gb_prop = gb_prop.groupby('bgOperatorName')['UploadKey'].count().reset_index()\n",
    "gb_prop.columns = ['bgOperatorName','disclosure_cnt_prop']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac63e1-5268-4e0f-8c45-1667ca50a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = pd.merge(gb_full,gb_prop,on='bgOperatorName',how='left')\n",
    "mg = pd.merge(mg,gb1,on='bgOperatorName',how='left')\n",
    "mg = pd.merge(mg,newmg.reset_index(),on='op_common',how='left')\n",
    "mg = mg.rename({'UploadKey':'num_prop_records'},axis=1)\n",
    "mg = mg[mg.disclosure_cnt_prop>0]\n",
    "mg.disclosure_cnt_prop.fillna(0,inplace=True)\n",
    "mg['prop_perc'] = mg.disclosure_cnt_prop/mg.disclosure_cnt_all * 100\n",
    "mg.sort_values('disclosure_cnt_all',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215a43b-6ac3-483b-b6f2-bd1c98a0f617",
   "metadata": {},
   "source": [
    "# Suppliers\n",
    "\n",
    "Here we need to remove records that are systems approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481b5b7-18ac-44e0-afef-5eb1db8c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.CASNumber.str.lower().str.contains('listed below')\n",
    "c1 = df.IngredientName.str.lower().str.contains('listed below')\n",
    "print(f'Number of disclosures that are systems approach removed from consideration : {df[c1|c].UploadKey.unique().size}')\n",
    "upk = df[c1|c].UploadKey.unique().tolist()\n",
    "prop_df_sup = prop_df[~prop_df.UploadKey.isin(upk)]\n",
    "print(f'Number of disclosures in the supplier analysis: {prop_df_sup.UploadKey.unique().size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec57622-e9cc-43c0-971e-623a2eda5d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# records to exclude\n",
    "not_comp = ['MISSING','Listed Above']\n",
    "\n",
    "gb1 = prop_df_sup.groupby('bgSupplier')['Supplier'].agg(lambda x: x.value_counts().index[0])\n",
    "gb1 = gb1.reset_index()\n",
    "gb1.columns = ['bgSupplier','sup_common']\n",
    "mg = pd.merge(prop_df_sup,gb1,on='bgSupplier',how='left')\n",
    "\n",
    "mg = mg[~mg.sup_common.isin(not_comp)]\n",
    "\n",
    "ax = mg.groupby('sup_common')['UploadKey'].count()\\\n",
    "         .sort_values(ascending=False)[:15].plot.barh(figsize=(7,7))\n",
    "ax.tick_params(axis=\"y\", labelsize=14)\n",
    "ax.tick_params(axis=\"x\", labelsize=14)\n",
    "plt.xlabel('Number of records',fontsize=16);\n",
    "plt.ylabel('Supplier Company',fontsize=16);\n",
    "plt.title(f'Number of records declared proprietary, by supplier',fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95469e-3e58-4855-8039-e907da02379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_full = df.groupby(['bgSupplier'],as_index=False).size()\n",
    "gb_full.columns = ['bgSupplier','record_cnt_all']\n",
    "\n",
    "gb_prop = prop_df_sup.groupby(['bgSupplier'],as_index=False).size()\n",
    "gb_prop.columns = ['bgSupplier','record_cnt_prop']\n",
    "\n",
    "# gb_precs = prop_df.groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40861fb9-ad9e-4c1d-860c-6ec65bf1d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = pd.merge(gb_full,gb_prop,on='bgSupplier',how='left')\n",
    "mg = pd.merge(mg,gb1,on='bgSupplier',how='left')\n",
    "mg.record_cnt_prop.fillna(0,inplace=True)\n",
    "mg['prop_perc'] = mg.record_cnt_prop/mg.record_cnt_all * 100\n",
    "mg.sort_values('record_cnt_all',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b000f-4e87-4af1-a888-4a2a4ef34b9f",
   "metadata": {},
   "source": [
    "# MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b15ce9-99fd-43ad-898a-19a142c7aec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "import numpy as np\n",
    "\n",
    "import branca.colormap as cm\n",
    "linear = cm.LinearColormap(['green','yellow','red'], vmin=3., vmax=10.)\n",
    "linear\n",
    "\n",
    "def fix_county_names(df):\n",
    "    trans = {'mckenzie':'mc kenzie',\n",
    "             'dewitt':'de witt',\n",
    "             'mcclain':'mc clain',\n",
    "             'mcintosh':'mc intosh',\n",
    "             'mckean':'mc kean',\n",
    "             'mcmullen':'mc mullen'}\n",
    "    for wrong in trans.keys():\n",
    "        df.CountyName = np.where(df.CountyName==wrong,trans[wrong],df.CountyName)\n",
    "    return df\n",
    "\n",
    "def create_county_choropleth(data,\n",
    "                             start_loc=[40, -96],start_zoom = 6,\n",
    "                             custom_scale = [], plotlog = True,\n",
    "                             legend_name = 'Test legend',\n",
    "                             show_only_data_states=True,\n",
    "                             fill_color = 'YlOrRd',\n",
    "                             #popup_enabled=True, tooltip_enabled=False,\n",
    "                             fields = ['CountyName','orig_value'],\n",
    "                             aliases = ['County: ','data: ']):\n",
    "    fn = r\"counties.geojson\"\n",
    "    if len(data)<1:\n",
    "        print('No mappable data')\n",
    "        return\n",
    "    geojson = gpd.read_file(fn)\n",
    "    data['orig_value'] = data.value\n",
    "\n",
    "    geojson['StateName'] = geojson.ste_name.str.lower()\n",
    "    geojson['CountyName'] = geojson.coty_name.str.lower()\n",
    "    geojson = fix_county_names(geojson)\n",
    "    working = geojson[['StateName','CountyName','coty_code','geometry']]\n",
    "    #geojson = geojson.to_crs(5070)\n",
    "    working = pd.merge(working,data,on=['StateName','CountyName'],how='left')\n",
    "    #print(geojson.info())\n",
    "    if start_loc==[]:\n",
    "        start_loc = [geojson.geometry.centroid.x.mean(),geojson.geometry.centroid.y.mean()]\n",
    "    f = folium.Figure(width=600, height=400)\n",
    "    m = folium.Map(location= start_loc, tiles=\"openstreetmap\",\n",
    "                   zoom_start=start_zoom).add_to(f)\n",
    "    if plotlog:\n",
    "        working.value = np.log10(working.value+1)\n",
    "        legend_name = legend_name + ' (log transformed)'\n",
    "    working.orig_value.fillna('no data',inplace=True)\n",
    "    \n",
    "    if custom_scale==[]:\n",
    "        custom_scale = (working['value'].quantile((0,0.2,0.4,0.6,0.8,1))).tolist()\n",
    "    if show_only_data_states:\n",
    "        gb = data.groupby(['StateName','CountyName'],as_index=False)['value'].first()\n",
    "        datalst = []\n",
    "        for i,row in gb.iterrows():\n",
    "            datalst.append((row.StateName,row.CountyName))\n",
    "        wlst = []\n",
    "        working['tup'] = list(zip(working.StateName.tolist(),working.CountyName.tolist()))\n",
    "        geojson['tup'] = list(zip(geojson.StateName.tolist(),geojson.CountyName.tolist()))\n",
    "        \n",
    "#         working = working[working.StateName.isin(data.StateName.unique().tolist())]\n",
    "#         geojson = geojson[geojson.StateName.isin(data.StateName.unique().tolist())]\n",
    "#         c1 = working.CountyName.isin(data.CountyName.unique().tolist())\n",
    "#         c2 = working.StateName.isin(data.StateName.unique().tolist())\n",
    "#         c3 = geojson.CountyName.isin(data.CountyName.unique().tolist())\n",
    "#         c4 = geojson.StateName.isin(data.StateName.unique().tolist())\n",
    "        working = working[working.tup.isin(datalst)]\n",
    "        geojson = geojson[geojson.tup.isin(datalst)]\n",
    "    working.StateName = working.StateName.str.title()\n",
    "    working.CountyName = working.CountyName.str.title()\n",
    "    #print(f'States in geojson: {working.StateName.unique().tolist()}')\n",
    "\n",
    "    linear = cm.LinearColormap(['green','yellow','red'], vmin=3., vmax=10.)\n",
    "    linear\n",
    "    \n",
    "    folium.Choropleth(\n",
    "                geo_data=geojson,\n",
    "                data=working,\n",
    "                columns=['coty_code', 'value'],  #Here we tell folium to get the fips and plot values for each state\n",
    "                key_on='feature.properties.coty_code',\n",
    "                threshold_scale=custom_scale, #use the custom scale we created for legend\n",
    "                #fill_color='YlOrRd',\n",
    "                fill_color=fill_color,\n",
    "                nan_fill_color=\"gainsboro\", #Use white color if there is no data available for the area\n",
    "                fill_opacity=0.7,\n",
    "                line_opacity=0.4,\n",
    "                line_weight=0.4,\n",
    "                legend_name= legend_name, #title of the legend\n",
    "                highlight=True,\n",
    "                line_color='black').add_to(m) \n",
    "    \n",
    "    folium.features.GeoJson(\n",
    "                data=working,\n",
    "                name='',\n",
    "                smooth_factor=2,\n",
    "                style_function=lambda x: {'color':'black','fillColor':'transparent','weight':0.5},\n",
    "                popup=folium.features.GeoJsonPopup(\n",
    "                    fields=fields,\n",
    "                    aliases=aliases, \n",
    "                    localize=True,\n",
    "                    sticky=False,\n",
    "                    labels=True,\n",
    "                    style=\"\"\"\n",
    "                        background-color: #F0EFEF;\n",
    "                        border: 2px solid black;\n",
    "                        border-radius: 3px;\n",
    "                        box-shadow: 3px;\n",
    "                    \"\"\",\n",
    "                    max_width=800,),\n",
    "                        highlight_function=lambda x: {'weight':3,'fillColor':'grey'},\n",
    "                    ).add_to(m)  \n",
    "\n",
    "    \n",
    "\n",
    "# fit_bounds needs work: https://stackoverflow.com/questions/58162200/pre-determine-optimal-level-of-zoom-in-folium\n",
    "#     sw = data[['bgLatitude', 'bgLongitude']].min().values.tolist()\n",
    "#     ne = data[['bgLatitude', 'bgLongitude']].max().values.tolist()\n",
    "\n",
    "#     m.fit_bounds([sw, ne]) \n",
    "    display(f)\n",
    "\n",
    "\n",
    "def CountyMap(df):\n",
    "    state_list = df.bgStateName.unique().tolist()\n",
    "    #start_loc = get_geog_center(state_list)\n",
    "    #print(statename,start_loc)\n",
    "    cond = (df.loc_within_state=='YES')&(df.loc_within_county=='YES')\n",
    "    if cond.sum()==0:  # no valid fracks for this state\n",
    "        display(md('## No mappable fracks for this operator!'))\n",
    "        # display(md(f'Any data in this state set may be labeled incorrectly as {statename}'))\n",
    "        return\n",
    "    gb = df[cond].groupby(['bgStateName','bgCountyName',\n",
    "                                                   'UploadKey'],as_index=False)['bgCAS'].count()\n",
    "    gb = gb.groupby(['bgStateName','bgCountyName'],as_index=False)['UploadKey'].count().rename({'bgStateName':'StateName',\n",
    "                                                                                                'bgCountyName':'CountyName',\n",
    "                                                                                                'UploadKey':'value'},\n",
    "                                                                                                axis=1)    \n",
    "    zoom = 3.6\n",
    "    create_county_choropleth(gb,plotlog=True,#plotlog=True,\n",
    "                             custom_scale= [0,1,2,3,4],\n",
    "                             #start_loc=start_loc, # center of state's data\n",
    "                             legend_name='Number of FracFocus disclosures',\n",
    "                             start_zoom=zoom,fields=['StateName','CountyName','orig_value'],\n",
    "                             aliases=['State: ','County: ','Number Fracking disclosures: '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccbaa2-b4c7-4d40-b50e-c57b97756185",
   "metadata": {},
   "source": [
    "## Map by number of disclosures\n",
    "Does this color scheme support our points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8a538-0fee-42cd-833e-6616dd419448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CountyMap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bea3e6-f278-4e04-8d6e-a283b538b101",
   "metadata": {},
   "source": [
    "# make map with proprietary fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739ca30-3424-431a-89c5-6bfc06db5515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first make data frame with number of fracks by county\n",
    "gb_all = df.groupby(['bgStateName','bgCountyName','UploadKey'],as_index=False).size()\n",
    "gb_all = gb_all.groupby(['bgStateName','bgCountyName'],as_index=False)['UploadKey'].count()                    \n",
    "# gb_all\n",
    "\n",
    "# now make data frame with number of fracks with at least one prop. chem by county\n",
    "gb_prop = prop_df.groupby(['bgStateName','bgCountyName','UploadKey'],as_index=False).size()\n",
    "gb_prop = gb_prop.groupby(['bgStateName','bgCountyName'],as_index=False)['UploadKey'].count()                    \n",
    "gb_prop.columns = ['bgStateName','bgCountyName','UploadKey_prop']\n",
    "gb_prop\n",
    "\n",
    "mg = pd.merge(gb_all,gb_prop,on=['bgStateName','bgCountyName'],how='left')\n",
    "mg.UploadKey_prop.fillna(0,inplace=True)\n",
    "mg['perc_prop'] = mg.UploadKey_prop/mg.UploadKey *100\n",
    "#mg[mg.UploadKey>0].perc_prop.hist(bins=5)\n",
    "\n",
    "mg['prop_dev'] = mg.perc_prop - 82\n",
    "\n",
    "mg.UploadKey_prop.sum()/mg.UploadKey.sum() *100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f44d6-5fd6-4143-987d-b171ec3c5a3a",
   "metadata": {},
   "source": [
    "## %disclosures_with_proprietary: 4 simple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79a020-5040-4cd3-8104-58533f2c25de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CountyPropMap(df):\n",
    "    state_list = df.bgStateName.unique().tolist()\n",
    "    zoom = 3.6\n",
    "    df['value'] = df.perc_prop\n",
    "    # df['value'] = df.prop_dev\n",
    "    df['StateName'] = df.bgStateName\n",
    "    df['CountyName'] = df.bgCountyName\n",
    "    create_county_choropleth(df,plotlog=False,\n",
    "                             custom_scale=[0,25,50,75,100],\n",
    "                             fill_color='RdBu_r',\n",
    "                             #start_loc=start_loc, # center of state's data\n",
    "                             legend_name='Percent of disclosures with at least one proprietary record',\n",
    "                             start_zoom=zoom,fields=['bgStateName','bgCountyName','orig_value','UploadKey'],\n",
    "                             aliases=['State: ','County: ','% disc with proprietary: ','total num of disclosures'])\n",
    "    \n",
    "CountyPropMap(mg[mg.UploadKey>5].copy()) # county must have more than 5 FF disclosures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3d6db-bd7c-4763-844b-b182dacf15b3",
   "metadata": {},
   "source": [
    "# Compare to Trickey 2020\n",
    "state-wide proprietary rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6da0c3-941f-4ecb-82e6-6100f7b19fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gb_all = df.groupby(['bgStateName','UploadKey'],as_index=False).size()\n",
    "gb_all = gb_all.groupby('bgStateName',as_index=False)['UploadKey'].count()\n",
    "gb_all.columns = ['bgStateName','num_all_disc']\n",
    "\n",
    "gb_prop = df[df.bgCAS=='proprietary'].groupby(['bgStateName','UploadKey'],as_index=False).size()\n",
    "gb_prop = gb_prop.groupby('bgStateName',as_index=False)['UploadKey'].count()\n",
    "gb_prop.columns = ['bgStateName','num_prop_disc']\n",
    "\n",
    "mg = pd.merge(gb_all,gb_prop,on='bgStateName',how='left')\n",
    "mg.num_prop_disc.fillna(0,inplace=True)\n",
    "\n",
    "mg['state_prop_percent'] = mg.num_prop_disc/mg.num_all_disc *100\n",
    "mg.sort_values('num_all_disc',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a7285-9d0c-4ad4-90a6-b12b03c35151",
   "metadata": {},
   "source": [
    "# ambiguousID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd5f07-689b-4e4e-9624-ce91eac0fc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amb_df = df[df.bgCAS=='ambiguousID'].copy()\n",
    "amb_df['has_mass'] = amb_df.calcMass>0\n",
    "amb_df.has_mass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc18b6-4a89-467c-bc06-5ea5e62e996a",
   "metadata": {},
   "source": [
    "## what are the ambiguousID `IngredientName`s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec52b5-ae78-4f27-8332-11bcef7973ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amb_df[amb_df.has_mass].IngredientName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b039b1-604d-48c7-81a4-0f8324500930",
   "metadata": {},
   "source": [
    "## what are the ambiguousID `IngredientName`s  (Just the big ones)\n",
    "Mostly water-type things.  A handful of proppants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca9471-e7c8-47ef-99fe-84d22ec39484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amb_df[amb_df.calcMass>100000].IngredientName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a93a3-c621-4f5a-b11a-5a4e40daf821",
   "metadata": {},
   "source": [
    "# AmbiguousID Proppants: Number and mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8a0d0-210d-4f4f-b5ce-15f8038870a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proppants = ['silica substrate',\n",
    " 'remainder is made up of various other oxides and trace elements, of which cao, mgo, and fe2o3 are the largest percentages',\n",
    " 'cyrstalline silica','mix of various oxides (cao, mgo, and fe2o4','resin coated fracturing proppant',\n",
    " '40/70 ppc','mix of various oxides (cao, mgo, and fe2o3',\n",
    " 'mix of various oxides (cao, mgo, and fe203','mix of various oxides (cao, mgo, and fe204',\n",
    " 'crystalline silica, quartz','100 mesh sand','ceramic',\n",
    " '20/40 pc','crystalline silica,quartz','aluminum oxide','nfidb:sand-200 mesh silica',\n",
    " 'nfidb:200 mesh ssa-1','non-crystalline silica (impurity)',\n",
    " 'silica in form of quartz','40/70 white','mulite']\n",
    "c = amb_df.IngredientName.isin(proppants)\n",
    "\n",
    "print(f'Number of ambiguousID proppants = {len(amb_df[c])}; calculated mass= {round(amb_df[c].calcMass.sum(),0):,} lbs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f77b4b-dd2b-4916-b2b0-58a1e26e4cc1",
   "metadata": {},
   "source": [
    "## Likely water records in ambiguousID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e322034-827a-4663-9376-c45610aa6859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = amb_df.IngredientName.str.contains('water')\n",
    "\n",
    "print(f'Number of ambiguousID **water** = {len(amb_df[c])}; calculated mass= {round(amb_df[c].calcMass.sum(),0):,} lbs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f65632-e4a1-479c-bffb-c96349ce6e82",
   "metadata": {},
   "source": [
    "# Proprietary plot (added back 8/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9c4d9-a55f-4591-96b6-71fd421a634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "def proprietary_plot(df,plot_title='TEST',minyr=2014,maxyr=2022):\n",
    "    # df = df.copy()\n",
    "    df['year'] = df.date.dt.year\n",
    "    df = df[df.year<=maxyr]\n",
    "    df = df[df.year>=minyr]\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['propCut'] = pd.cut(mg.percProp,right=False,bins=[0,0.0001,10,25,50,101],\n",
    "                          labels=['no proprietary claims','up to 10% proprietary claims',\n",
    "                                  'between 10 and 25% proprietary claims',\n",
    "                                  'between 25 and 50% proprietary claims',\n",
    "                                  'greater than 50% proprietary claims'])\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    # tmp = mg.groupby(['year','propCut'],as_index=False).size()\n",
    "    # print(tmp.pivot(index='year',columns='propCut',values='size'))\n",
    "    \n",
    "    out = mg.drop(['date','UploadKey'],axis=1)\n",
    "    t = out[out.numvalid>0].groupby(['year','propCut'],as_index=False)['numvalid'].count()\n",
    "    sums = t.groupby('year',as_index=False)['numvalid'].sum().rename({'numvalid':'tot'},axis=1)\n",
    "    t = pd.merge(t,sums,on='year',how='left')\n",
    "    t['PercentProp'] = t.numvalid/t.tot *100\n",
    "\n",
    "    piv = t.pivot(index='year', columns='propCut', values='PercentProp')\n",
    "    \n",
    "    with pd.option_context(\"display.float_format\", \"{:,.1f} %\".format):\n",
    "        iShow(piv)\n",
    "\n",
    "    ax = piv.plot.area(figsize=(12,7),ylim=(0,100),xlim=(minyr,maxyr),colormap='Reds')\n",
    "    ax.set_title(f'Percentage of valid records that are Trade Secret claims at the disclosure level', fontsize=16)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[::-1], labels[::-1], title='Disclosure Proprietary\\nPercentage class\\n',\n",
    "              loc='upper left',bbox_to_anchor=(1, 1))\n",
    "    ax.set_ylabel('Percentage of disclosures', fontsize=16)\n",
    "    ax.set_xlabel('Year', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.suptitle(f'{plot_title}',fontsize=24)\n",
    "\n",
    "    gb = df.groupby(['year','UploadKey'],as_index=False)['bgCAS'].count()\n",
    "    gb = gb.groupby('year',as_index=False)['UploadKey'].count()#.rename({'UploadKey':'number of disclosures'},axis=1)\n",
    "    s = 'Number of disclosures by year:\\n\\n'\n",
    "    for i,row in gb.iterrows():\n",
    "        s+= f'   {row.year}: {row.UploadKey:7,} \\n'\n",
    "    at2 = AnchoredText(s,\n",
    "                       loc='lower left', prop=dict(size=10), frameon=False,\n",
    "                       bbox_to_anchor=(1., 0.),\n",
    "                       bbox_transform=ax.transAxes\n",
    "                       )\n",
    "    at2.patch.set_boxstyle(\"square,pad=0.\")\n",
    "    ax.add_artist(at2)\n",
    "\n",
    "    \n",
    "# test = 'pennsylvania'\n",
    "# variable = 'bgStateName'\n",
    "testtitle = 'Trade Secret frequency'\n",
    "proprietary_plot(df,testtitle,minyr=2014,maxyr=2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d298652-fdcf-4ff8-879e-e1b15efc5a83",
   "metadata": {},
   "source": [
    "###  Break up the categories for plotting and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e3951-c024-4f30-8aa1-bb2a97d2cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def new_plot(df, category='no proprietary designations'):\n",
    "    df['year'] = df.date.dt.year\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['propCut'] = pd.cut(mg.percProp,right=False,bins=[0,0.0001,10,25,50,101],\n",
    "                          labels=['no proprietary designations','up to 10% proprietary designations',\n",
    "                                  'between 10 and 25% proprietary designations',\n",
    "                                  'between 25 and 50% proprietary designations',\n",
    "                                  'greater than 50% proprietary designations'])\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    \n",
    "    out = mg.drop(['date','UploadKey'],axis=1)\n",
    "    t = out[out.numvalid>0].groupby(['year','propCut'],as_index=False)['numvalid'].count()\n",
    "    sums = t.groupby('year',as_index=False)['numvalid'].sum().rename({'numvalid':'tot'},axis=1)\n",
    "    t = pd.merge(t,sums,on='year',how='left')\n",
    "    t['PercentProp'] = t.numvalid/t.tot *100\n",
    "\n",
    "    g = sns.FacetGrid(t,col='propCut',col_wrap=2,height=4)\n",
    "    g.map(sns.regplot,'year','PercentProp')\n",
    "    for cat in mg.propCut.unique().tolist()[:-1]:\n",
    "        print(f'Regression stats for <{cat}>')\n",
    "        subdf = t[t.propCut==cat]\n",
    "        print(stats.linregress(subdf.year,subdf.PercentProp),'\\n')\n",
    "new_plot(df,'no proprietary designations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e19216-d1f5-4280-adc7-af4529c0065f",
   "metadata": {},
   "source": [
    "### Try analysis of just mean proportion over years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb42eb-3abb-4824-9f77-6f95ec486289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_plot(df):\n",
    "    df['year'] = df.date.dt.year\n",
    "    prop = df.bgCAS=='proprietary'\n",
    "    gb = df[prop].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numprop'},axis=1)\n",
    "    gb1 = df[df.is_valid_cas].groupby('UploadKey',as_index=False)['bgCAS'].count().rename({'bgCAS':'numvalid'},axis=1)\n",
    "    gb2 = df.groupby('UploadKey',as_index=False)['date'].first()\n",
    "    mg = pd.merge(gb2,gb,on='UploadKey',how='left')\n",
    "    mg = pd.merge(mg,gb1,on='UploadKey',how='left')\n",
    "    mg.fillna(0,inplace=True) # there will be disclosures with 0 proprietary; need to fill\n",
    "    mg['percProp'] = (mg.numprop / mg.numvalid) * 100\n",
    "\n",
    "    mg['year'] = mg.date.dt.year\n",
    "    ygb = mg.groupby('year',as_index=False)['percProp'].mean()\n",
    "\n",
    "    print(f'Regression stats')\n",
    "    print(stats.linregress(ygb.year,ygb.percProp),'\\n')\n",
    "    sns.regplot(data=ygb,x='year',y='percProp',scatter=True)\n",
    "single_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd69ee8-9990-480c-bccd-5ee0d2ad1f9b",
   "metadata": {},
   "source": [
    "## Oct 2023.  Median water volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06392be-b25b-4cb3-9914-61677e599e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import log10, floor\n",
    "def round_sig(x, sig=2,guarantee_str=''):\n",
    "    try:\n",
    "        if abs(x)>=1:\n",
    "            out =  int(round(x, sig-int(floor(log10(abs(x))))-1))\n",
    "            return f\"{out:,d}\" # does the right thing with commas\n",
    "        else: # fractional numbers\n",
    "            return str(round(x, sig-int(floor(log10(abs(x))))-1))\n",
    "    except: \n",
    "        if guarantee_str:\n",
    "            return guarantee_str\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ebb69-21b6-4684-bedb-16442e95227a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pylab import gca, mpl\n",
    "df['year'] = df.date.dt.year\n",
    "gb = df.groupby(['year','UploadKey'],as_index=False)['TotalBaseWaterVolume'].first()\n",
    "gb1 = gb.groupby('year',as_index=False)['TotalBaseWaterVolume'].median()\n",
    "gb1.plot('year','TotalBaseWaterVolume',ylim=(0,16500000),title='Median volume water by year')\n",
    "ax = gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "gb1['rounded'] = gb1.TotalBaseWaterVolume.map(lambda x: round_sig(x,3))\n",
    "gb1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
